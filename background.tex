\chapter{Background}
\label{cha:background}
\vspace{0.4 cm}

This chapter contains background information on the most relevant topics in this thesis: Large Language Models (LLMs), Software Testing and Test Oracle, Automated Test Generation.
Software development has evolved into a cornerstone of modern society, powering an array of applications from communication and commerce to healthcare and entertainment. This ubiquity underscores the critical importance of software quality and reliability. In the quest for robust software, software testing plays a pivotal role. The effectiveness of testing largely hinges on the concept of a "test oracle," which serves as a benchmark for assessing whether software behaves as expected.

\section{Large Language Models (LLMs)}
\label{sec:llms}
\vspace{0.2 cm}

Large language models (LLMs) represent a remarkable breakthrough in deep learning for natural language processing. They are capable of understanding and generating text in a manner that closely resembles human language comprehension. Behind the scenes, these LLMs are powered by extensive transformer models, which serve as the driving force behind their impressive capabilities.

While humans perceive text as collections of words, sentences, and documents, computers view text as mere sequences of characters. To bridge this gap, recurrent neural networks were initially employed to process text, operating one word or character at a time. However, they often struggled with retaining information from the beginning of a sequence when reaching the end.

In 2017, Vaswani et al. introduced the "Attention is All You Need"\cite{vaswani_attention_2017} paper, which laid the foundation for the transformer model. This revolutionary model leverages the attention mechanism, allowing it to process entire sentences or paragraphs at once, rather than one word at a time. This shift enables the transformer model to grasp the context of a word more effectively, leading to superior language understanding. Many cutting-edge language processing models are now based on transformers.

To utilize a transformer model for text processing, the first step involves tokenizing the text into a sequence of words\cite{}. These tokens are then encoded as numerical values and transformed into embeddings, which are vector representations preserving the meaning of the tokens. Subsequently, the transformer's encoder transforms these token embeddings into a context vector, encapsulating the essence of the entire input. Using this vector, the transformer's decoder generates output based on contextual clues. For example, providing the original input as a clue allows the decoder to produce the next word in a sentence naturally. This process can be repeated to generate entire paragraphs, starting from an initial sentenceâ€”a method known as autoregressive generation\cite{}. Large language models utilize transformer models capable of handling lengthy input texts, generating complex concepts with their extensive encoder and decoder layers.

Large language models, such as the GPT-3 model supporting ChatGPT, are built on an extensive scale. They are so massive that they cannot run on a single computer and are typically offered as services through APIs or web interfaces. These models are trained on vast amounts of text data from the internet, encompassing books, articles, websites, and diverse sources\cite{}. During training, the model learns the statistical relationships between words, phrases, and sentences, enabling it to generate coherent and contextually relevant responses to prompts or queries.

Drawing from this wealth of text, GPT-3, for example, exhibits multilingual capabilities and possesses knowledge across various topics. This versatility allows it to produce text in various styles. Leveraging this, large language models can perform tasks like translation, text summarization, question answering and sentiment analysis\cite{}. They have become valuable tools for automating language-related tasks, simplifying content generation, and enhancing human-computer interaction. Beyond their natural language capabilities, LLMs are increasingly being explored in the realm of software engineering. Their proficiency in understanding programming languages and code-related contexts has opened doors to innovative applications in software development, code generation, and, notably, software testing\cite{}.

Here are some of the key concepts of LLMs used in this thesis:
\begin{enumerate}
    \item \textbf{Prompts:} A prompt is a sequence of words or tokens provided to an LLM to generate a specific response. It serves as an instruction or query that guides the model in producing coherent and contextually relevant output. Crafting effective prompts is crucial in eliciting desired responses from LLMs, and it involves considering the language style, structure, and context that the model has been trained on.

    \item \textbf{Tokens:} In the context of natural language processing, a token is a unit of text that LLMs operate on. A token isn't necessarily a complete word; it might be a smaller unit such as a character, a portion of a word, or even a larger entity like an entire phrase. For example, the sentence "LLMs are amazing!" would be tokenized into ["LLMs", " are", " amazing", "!"].

    \item \textbf{LLM Configurations:} LLMs come with various configurations that can be adjusted to influence their behavior during text generation. Some key configurations include:

        \begin{itemize}
            \item \textbf{Temperature:} A parameter controlling the randomness of the generated output. Higher values (e.g., 1.0) produce more diverse but potentially less coherent text, while lower values (e.g., 0.7) result in more focused and deterministic responses.

            \item \textbf{n\_predict:} The number of tokens to predict in a single generation step. Larger values can capture more context but may lead to slower generation.

            \item \textbf{top\_p (nucleus sampling):} A probability threshold for sampling the next token. It helps in controlling the diversity of generated responses by limiting the choices to the most probable tokens.

            \item \textbf{top\_k:} An alternative to top\_p, top\_k specifies the number of most probable tokens to consider for sampling.

            \item \textbf{n\_batch:} The number of parallel sequences processed in each batch during generation.
            
            \item \textbf{repeat\_penalty:} A penalty applied to repeated tokens to encourage diversity in generated sequences.
    
            \item \textbf{repeat\_last\_n:} A parameter controlling the likelihood of repeating the last n tokens in the generated output.
        \end{itemize}

    These configurations provide users with fine-grained control over the trade-off between creativity and coherence in the generated text, allowing customization based on specific use cases.
    
\end{enumerate}

\section{Software Testing and Test Oracle}
\label{sec:software_testing_and_oracle}
\vspace{0.2 cm}

Software testing emerges as the sentinel, guarding against the unseen complexities that may lurk within code. It is the systematic process of evaluating software to ensure that it behaves as intended and meets the specified requirements. Let us go through some of the key components and concepts of software testing.

\begin{enumerate}
  \item \textbf{Unit testing:} Unit testing is the bedrock of software quality assurance, focusing on evaluating individual units or components of code in isolation. Each unit, typically a function or method, undergoes a battery of test cases designed to validate its correctness, functionality, and conformity to specifications. The goal is to ensure that these units operate as intended and integrate seamlessly within the broader software architecture. Unit testing facilitates early bug detection, fosters code maintainability, and forms an integral part of the test-driven development (TDD) methodology.
  \item \textbf{Test Case:} A test case is a set of predefined actions and inputs appropriately crafted to assess a specific aspect of software functionality. Test cases serve as the evaluative criteria, scrutinizing the software under different scenarios. A JUnit\cite{noauthor_junit_nodate} test case typically follows a structure that includes the following components:

    \begin{itemize}
        \item \textbf{Annotations:} \\
        - \textit{@Test}: Indicates that the method is a test method. \\
        - Other annotations like \textit{@Before, @After, @BeforeClass}, and \textit{@AfterClass} may also be used for setup and teardown operations.

        \item \textbf{Test Method:} \\
        - Contains the actual test logic that needs to be verified. \\
        - It is annotated with \textit{@Test}. \\
        - It often includes assertions to check whether the expected results match the actual results.

        \item \textbf{Setup and Teardown Methods (Optional):} \\
        - \textit{@Before}: Executed before each test method. It is used for setting up preconditions. \\
        - \textit{@After}: Executed after each test method. It is used for cleaning up resources. \\
        - \textit{@BeforeClass}: Executed once before any of the test methods in the class. It is used for expensive setup. \\
        - \textit{@AfterClass}: Executed once after all the test methods in the class have been run. It is used for cleanup after all tests.

        \item \textbf{Assertions:} \\
        - Used to verify whether the expected outcomes match the actual outcomes. \\
        - Commonly used methods\cite{noauthor_assert_nodate} include "assertEquals", "assertTrue", "assertFalse", "assertNotNull", "assertNull", etc.
    \end{itemize}
  
  \item \textbf{Test Suite:} A collection of individual test cases forms a test suite. Test suites encompass diverse scenarios, covering a spectrum of functionalities to comprehensively evaluate the software.
  \item \textbf{Bug:} A bug, often referred to as a defect or issue, denotes an unintended and erroneous behavior within the software. The goal of testing is to unearth these bugs, enabling developers to rectify them before the software reaches end-users.
  \item \textbf{Test Oracle:} The Test Oracle is a critical entity that determines whether the software under examination 
  has exhibited the expected behavior. Think of it as the judge in a courtroom, rendering a verdict on whether the software stands acquitted or if discrepancies, i.e., bugs, have been found. It encapsulates the specifications, requirements, and expected results against which the software's performance is evaluated.
  \item \textbf{Line coverage:} Line coverage stands as a quantitative measure gauging the extent to which individual lines of code are exercised during testing. It represents the ratio of executed lines to the total lines in the codebase, providing insights into the effectiveness of test suites. While line coverage offers a valuable metric for assessing code exploration, it doesn't guarantee comprehensive testing, as it solely focuses on the presence or absence of executed lines, overlooking potential edge cases and conditional scenarios.
  \item \textbf{Mutation testing:} Mutation testing injects controlled defects, or mutations, into the codebase to evaluate the efficacy of test suites in detecting these alterations. Each mutation introduces a subtle change, simulating a potential bug. The ability of test suites to identify and flag these mutations reflects their robustness. This technique goes beyond traditional coverage metrics, assessing the quality of tests by evaluating their sensitivity to subtle code variations.
  \item \textbf{Mutation coverage:} Mutation coverage quantifies the proportion of mutations that test suites successfully detect. It serves as a qualitative measure, indicating the thoroughness of testing in identifying and isolating potential defects. A high mutation coverage suggests that the test suite is adept at pinpointing code alterations, enhancing confidence in the software's resilience against subtle bugs and ensuring comprehensive testing across diverse scenarios.
  \item \textbf{Test strength:} Test strength encapsulates the effectiveness of a test suite in uncovering defects and ensuring software reliability. It considers factors such as the diversity of test cases, their ability to explore different code paths, and their capacity to reveal potential vulnerabilities. A robust test suite exhibits high test strength, providing a solid defense against bugs and contributing to the overall quality assurance strategy.
  
\end{enumerate}



\section{Automated Test Generation}
\label{sec:automated_test_generation}
\vspace{0.2 cm}

Historically, test oracles have been constructed manually by human testers, inspecting software outputs against expected results. While effective, this approach is labor-intensive and prone to human error. Automation techniques have been developed to alleviate this burden. Automation not only reduces the resources required for testing but also enhances test coverage by enabling the generation of a large number of test cases with minimal human intervention.

Automated test generation techniques leverage various algorithms and heuristics\cite{} to generate test cases automatically. These techniques can be categorized into model-based, search-based, and \\
specification-based approaches. Model-based methods create models of the software's behavior, while search-based approaches explore the input space to find test cases that trigger specific conditions. Specification-based methods rely on formal specifications to guide test case generation.\cite{}

\textbf{The Oracle Problem:}
In the context of automated testing, the Oracle Problem refers to the challenge of accurately determining expected outcomes. If the criteria for expected behavior are flawed, the automated testing tool may inadvertently accept erroneous behaviors as correct. 
Software systems are dynamic, subject to frequent updates, enhancements, and modifications. These changes ripple through the entire ecosystem, impacting functionalities and potentially altering expected outcomes. Maintaining a static set of expected outcomes becomes arduous. As the software evolves, the Oracle must dynamically adapt to these changes, requiring a level of agility that traditional testing approaches struggle to achieve. Defining precise criteria for expected behavior in complex software systems is inherently intricate. It involves considering a multitude of variables, interactions, and potential scenarios, often surpassing human capacity for manual formulation. Traditional automated testing tools often rely on predefined algorithms or criteria to determine expected outcomes. Crafting algorithms that encapsulate the nuanced behavior of diverse functionalities poses a considerable technical challenge, especially in scenarios where the desired outcome may not have a deterministic definition. The Oracle must possess adaptability and resilience to handle unforeseen circumstances. Traditional oracles, lacking the ability to dynamically adapt, may falter in scenarios where predefined criteria are insufficient to encompass the entire spectrum of possible outcomes\cite{}.

Understanding the technical nuances of the Oracle Problem sets the stage for exploring innovative solutions, such as leveraging Large Language Models (LLMs), to navigate the intricacies of automated testing in contemporary software landscapes.