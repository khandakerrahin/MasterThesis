\chapter{State of the Art}
\label{cha:state_of_the_art}
\vspace{0.4 cm}

In this chapter, we perform a comprehensive exploration of the current state of the art within the domain of automated test oracle generation and the application of Large Language Models (LLMs) in software testing. We initiate our journey by introducing the foundational concepts and standards that underpin the representation of software testing data, with a specific focus on the intricacies of test oracles. Subsequently, we delve into an extensive survey of cutting-edge technologies and methodologies proposed in the literature for automated test oracle generation. This survey encompasses a wide spectrum of implementations and use cases, offering a panoramic view of the advancements in this field. As our journey unfolds, we shine a spotlight on two pivotal subjects that have garnered significant attention and innovation in the research community: Oracle generation and Unit Test Generation. These topics are not merely explored but dissected in dedicated subsections, elucidating their relevance and implications in the context of test oracle generation. Furthermore, we narrow our gaze to scrutinize the intricate nuances of three distinct yet interrelated use cases, each of paramount importance: automated test oracle generation for functional testing, regression testing, and security testing. These dedicated sections unravel the intricacies of generating test oracles that cater to the specific requirements and challenges posed by each testing domain. By the end of this chapter, a clear and comprehensive context will be established, providing the foundation upon which our proposed system for automated test oracle generation is developed. The synthesis of insights from literature, advancements in technology, and an understanding of the unique challenges in the realm of software testing will pave the way for a novel and effective approach that leverages Large Language Models to elevate the field of software quality assurance.


\section{Methodology for Identifying Relevant Papers}
\label{sec:papaer_survey_methodology}
\vspace{0.2 cm}

In this section, we describe the methodology that we followed to identify relevant papers for our study. We conducted this survey to collect scientific literature at the crossroads of software testing and large language models. Our search strategy aimed for a broad search string that encompasses various ways researchers might mention work related to oracle generation with large language models, as explained below:

\begin{enumerate}[label=\textbf{\arabic*.}]
  \item\textbf{Initial search and first-stage filtering.} We began our search in the SciVerse Scopus digital library by using the following search string. We chose all papers whose title, abstract, or keywords aligned with any of the keywords in the query:

  \textit{( TITLE-ABS-KEY( "software testing" OR "software verification" OR "software validation" OR "test case generation" OR "test suite generation" OR "test generation" OR "mutation testing" OR "fuzz testing" OR "property-based testing" OR "symbolic execution" OR "oracle" OR "assertion" ) AND TITLE-ABS-KEY ( "language model*" OR "transformer model*" ) )}

  We started our search with a broad query in the SciVerse Scopus digital library and got 191 studies initially. We performed this search on 18 May 2023, 16:51:58. Since the query was broad, the initial set had papers not related to Software Testing and Large Language Models. It included various types of artifacts like research articles, editorials, standards, and welcome messages. To refine our selection, we manually reviewed titles and abstracts, removing papers clearly unrelated to Software Testing and LLMs, as well as irrelevant artifacts. After this, we ended up with a final set of 37 studies.
  
  \item\textbf{Selection criteria.} We further filtered the collected papers based on certain rules to keep only the ones directly related to software testing with large language models. We excluded papers that didn't focus on either software testing or large language models. The papers we kept had to meet specific criteria:
      \begin{itemize}
            \item \textbf{Inclusion Criterion 1:} The study should involve defining, applying, or experimenting with software testing solutions using large language models.
            \item \textbf{Inclusion Criterion 2:} It presents an empirical or experimental study exploring the use of LLMs in software testing practices.
            \item \textbf{Inclusion Criterion 3:} The paper applies LLMs to various tasks within the software testing lifecycle.
            \item \textbf{Inclusion Criterion 4:} The study should have undergone peer review.
            \item \textbf{Inclusion Criterion 5:} The study should be written in English.
            
      \end{itemize}

  We excluded papers that meet at least one of the following exclusion criteria:
        \begin{itemize}
            \item \textbf{Exclusion Criterion 1:} Papers that don't involve software testing tasks, such as code comment generation.
            \item \textbf{Exclusion Criterion 2:} Papers that don't use Large Language Models (LLMs).
            \item \textbf{Exclusion Criterion 3:} Papers mentioning LLMs only in future work or discussions rather than actively using LLMs in their approach.
            \item \textbf{Exclusion Criterion 4:} Studies conducted before 2015.
            \item \textbf{Exclusion Criterion 5:} Secondary or tertiary studies, such as systematic literature reviews and surveys.
            \item \textbf{Exclusion Criterion 6:} Studies not available as full-text.
      \end{itemize}
At this stage, we ended up with a total of 8 papers related to our topic.

  \item\textbf{Snowballing.} We finalized our study selection using a snowballing approach. First, we conducted a thorough backward snowballing, examining references in the chosen studies and adding relevant papers from major databases such as SciVerse Scopus, IEEEXplore, and ACM DL. Next, we performed a partial forward snowballing, selecting the 10\% most cited papers and the top 10\% in terms of normalized citations. This led us to identify four highly popular studies. We then considered all papers citing these popular studies, resulting in 12 potentially relevant studies. After applying inclusion and exclusion criteria, we added three new papers, concluding with a set of 11 studies presented in Table \ref{tab:collected_papers} for our survey.
\end{enumerate}

\begin{table}[H]
\centering


    \begin{tabular}{c|c|c|c|c}
        \textbf{ID} & \textbf{Topic} & \textbf{Paper Title} & \textbf{Year} & \textbf{Reference}\\
        \hline 
        1 & \scriptsize\textsc{Oracle Generation} & \scriptsize\textsc{TOGA: A Neural Method for Test Oracle Generation} & 2022 & \cite{gabriel_ryan_toga_2022}\\
        
        2 & \scriptsize\textsc{Oracle Generation} & \scriptsize\textsc{Generating accurate assert statements for unit test} & 2022 & \cite{tufano_generating_2022}\\
        & & \scriptsize\textsc{cases using pretrained transformers} & & \\
        
        3 & \scriptsize\textsc{Oracle Generation} & \scriptsize\textsc{Automated assertion generation via information} & 2022 & \cite{yu_automated_2022}\\
        & & \scriptsize\textsc{retrieval and its integration with deep learning} & & \\
        
        4 & \scriptsize\textsc{Oracle Generation} & \scriptsize\textsc{Learning Deep Semantics for Test Completion} & 2023 & \cite{nie_learning_2023}\\

        5 & \scriptsize\textsc{Oracle Generation} & \scriptsize\textsc{Towards More Realistic Evaluation for} & 2023 & \cite{liu_towards_2023}\\
        & & \scriptsize\textsc{Neural Test Oracle Generation} & & \\
        
        6 & \scriptsize\textsc{Unit Test Generation} & \scriptsize\textsc{Unit Test Case Generation with Transformers} & 2021 & \cite{tufano_unit_2021}\\
        & & \scriptsize\textsc{and Focal Context} & & \\
        
        7 & \scriptsize\textsc{Unit Test Generation} & \scriptsize\textsc{Adaptive Test Generation Using a Large} & 2023 & \cite{schafer_adaptive_2023}\\
        & & \scriptsize\textsc{Language Model} & & \\
        
        8 & \scriptsize\textsc{Unit Test Generation} & \scriptsize\textsc{ChatUniTest: a ChatGPT-based automated unit} & 2023 & \cite{xie_chatunitest_2023}\\
        & & \scriptsize\textsc{test generation tool} & & \\
        
        9 & \scriptsize\textsc{Unit Test Generation} & \scriptsize\textsc{No More Manual Tests? Evaluating and Improving} & 2023 & \cite{yuan_no_2023}\\
        & & \scriptsize\textsc{ChatGPT for Unit Test Generation} & & \\
        
        10 & \scriptsize\textsc{Unit Test Generation} & \scriptsize\textsc{ChatGPT vs SBST: A Comparative Assessment of} & 2023 & \cite{tang_chatgpt_2023}\\
        & & \scriptsize\textsc{Unit Test Suite Generation} & & \\
        
        11 & \scriptsize\textsc{Unit Test Generation} & \scriptsize\textsc{CodaMosa: Escaping Coverage Plateaus in Test} & 2023 & \cite{lemieux_codamosa_2023}\\
        & & \scriptsize\textsc{Generation with Pre-trained Large Language Models} & & \\
        
    \end{tabular}

\caption{Details of the collected papers.}
\label{tab:collected_papers}
\end{table}

\section{Oracle Generation}
\label{sec:soa_oracle_generation}
\vspace{0.2 cm}

Gabriel Ryan et al. introduce TOGA\cite{gabriel_ryan_toga_2022}, an innovative contribution to the domain of Test Oracle Generation. This comprehensive framework comprises distinct components, including an Exceptional Oracle Classifier, trained on Methods2Test\cite{noauthor_unit_2023}, an Assertion Oracle Ranker trained on Atlas, and Candidate Assertion Set Generation, aided by both a Global Constant Dictionary and a Local Dictionary. TOGA's integration with EvoSuite enhances its versatility, providing a test prefix and unit context. The authors address pivotal research questions, demonstrating the effectiveness of TOGA in both grammar representation and the inference of assertions and exceptional behavior. Their results showcase TOGA's accuracy, surpassing existing methods in oracle inference and revealing its potential to uncover a substantial number of real-world bugs, including those missed by other approaches. The contributions encompass the introduction of a transformer-based approach, adapted datasets, and an end-to-end integration with EvoSuite, all of which significantly advance the field of test oracle generation. While the study acknowledges potential threats to validity, the TOGA framework's multifaceted capabilities demonstrate a promising direction for automated test oracle generation in software testing research.

Tufano et al. present an innovative contribution\cite{tufano_generating_2022} to Test Oracle Generation, focusing on the accurate generation of assert statements for unit test cases using pretrained transformers. Their framework, comprising the BART Transformer, English and Code Pretraining, and Asserts Finetuning, demonstrates significant advancements in this domain. Addressing key research questions, their approach excels when compared to ATLAS, achieving a remarkable 80\% relative improvement in top-1 accuracy. The study also explores the impact of pretraining on assert generation tasks, revealing substantial performance gains attributed to English and source code pretraining. The incorporation of the Focal Method contributes to a notable improvement in assert generation accuracy. Furthermore, the paper highlights the quality of the generated assert statements, demonstrating the model's ability to generate complex assertions akin to developer-authored ones. Notably, their approach can be employed to enhance automatically generated test cases, improving test coverage by augmenting existing cases. The dataset, drawn from a diverse range of sources including English text and GitHub, underscores the empirical rigor of their research. While addressing potential threats to validity, the paper underscores the potential of pretrained transformers in revolutionizing test oracle generation, promising substantial advancements in the generation of accurate assert statements and the augmentation of test cases.

Nie et al. introduce a novel approach TECO\cite{nie_learning_2023}, aimed at aiding developers in the efficient creation of test methods. Their groundbreaking approach, leverages code semantics and execution processes in machine learning models for code-related tasks. TECO, a transformer model trained on extensive code semantics data, demonstrates exceptional performance on the test completion task. It surpasses all baseline models by a significant margin, achieving an impressive exact-match accuracy of 17.61\%, a 29\% improvement over the best baseline. TECO not only generates compilable and runnable statements with high success rates but also excels in other metrics measuring lexical similarity. Moreover, the paper highlights TECO's impact on test oracle generation, showcasing an 82\% enhancement in exact-match accuracy over the prior state-of-the-art TOGA. Reranking by execution further enhances the model's ability to predict the next statement accurately, outperforming both TECO-noRr and CodeT5. The study also delves into the contribution of different types of code semantics, emphasizing their importance in improving prediction accuracy. The extensive evaluation relies on a substantial corpus of test methods from open-source projects, demonstrating the potential of TECO for various testing-related tasks. While recognizing usability and structured code representation as future considerations, the paper underscores the value of code semantics in enhancing the performance of large language models for code-related tasks. This work represents a significant stride in advancing the field of test completion and the broader domain of software testing research.

Liu et al. propose a solution that focuses on the evaluation of Neural Test Oracle Generation (NTOG)\cite{liu_towards_2023} approaches, specifically examining the widely-used TOGA approach. The authors identify inappropriate settings in existing evaluation methods and propose a more realistic evaluation method named TEval+. They investigate the impact of generating test prefixes from bug-fixed program versions on TOGA's performance and find that it significantly inflates the bug-finding metrics. When using TEval@buggy, TOGA's Precision is only 0.38\%, indicating the need for improvement. A straightforward baseline, NoException, outperforms TOGA in bug-finding and precision. The paper introduces an unsupervised ranking method to enhance TEval+, improving the cost-effectiveness of TOGA. The findings emphasize the importance of realistic evaluation metrics for NTOG approaches and propose seven rules of thumb for more practical assessments. The contributions include uncovering inappropriate settings, proposing TEval+ for more realistic evaluations, and introducing an effective ranking method. Threats to validity are addressed through rigorous experimental procedures and statistical analyses.

In the paper titled "Automated Assertion Generation via Information Retrieval and Its Integration with Deep Learning," Yu et al. present an innovative Information Retrieval (IR)-based assertion generation approach\cite{yu_automated_2022} and compare it with the latest Deep Learning (DL) approach, ATLAS. The IR-based technique, IRar, demonstrates remarkable efficacy by surpassing ATLAS in generating correct assertions. Among the 4,925 correct assertions generated by ATLAS, IRar produces 4,560 (92.59\%). The study delves into the nuances of correct and incorrect assertions, revealing the strengths of the IRar approach. Furthermore, the authors introduce two retrieved-assertion adaptation techniques, RAH and RANN, showcasing their impact on enhancing the IR-based approach's effectiveness. The paper assesses these techniques on practical assertion generation cases, demonstrating improvements in accuracy. Additionally, the authors propose an integration approach that combines DL-based and IR-based approaches, consistently outperforming individual methods in assertion generation effectiveness. The comprehensive dataset, ATLAS, enables a thorough evaluation. The contributions include pioneering an IR-based approach for assertion generation, introducing effective adaptation techniques, and emphasizing the competitive edge of IR-based solutions in software engineering tasks. The results, including percentages and metrics, underscore the practical implications of the findings. Threats to validity are carefully acknowledged, providing a nuanced understanding of the study's limitations.

\section{Unit Test Generation}
\label{sec:soa_unit_test_generation}
\vspace{0.2 cm}

In the paper titled "Adaptive Test Generation Using a Large Language Model," Schäfer et al. present TESTPILOT\cite{schafer_adaptive_2023}, an end-to-end test-generation technique based on Large Language Models (LLMs). Addressing multiple research questions, TESTPILOT achieves notable results. It attains passing tests ranging from 11.8\% to 74.4\%, with a median of 47.1\%, demonstrating its effectiveness in statement coverage. Furthermore, 58.5\% of generated tests are non-trivial, emphasizing the inclusion of meaningful assertions. The study delves into the characteristics of failing tests, pinpointing correctness errors and timeouts as common issues. The authors introduce a novel prompt refining technique, evaluating its impact on test effectiveness. Additionally, they explore the potential copying of generated tests from existing ones, revealing that TESTPILOT avoids exact duplication. The dataset includes 25 npm packages, showcasing TESTPILOT's adaptability across diverse domains. The paper's contributions include the introduction of an LLM-based test-generation technique and a thorough evaluation of TESTPILOT's performance on various aspects, providing valuable insights into the quality and effectiveness of generated tests. Threats to validity are carefully considered, ensuring a nuanced interpretation of the findings.

Tufano et al. introduce ATHENATEST\cite{tufano_unit_2021}, an innovative unit test case generation approach designed to enhance software testing in the development lifecycle. Unlike traditional methods that rely on coverage criteria, ATHENATEST leverages real-world focal methods and developer-written test cases. The approach adopts a sequence-to-sequence learning task, involving denoising pretraining on a vast unsupervised Java corpus and supervised finetuning for generating unit tests. The study explores the impact of natural language and source code pretraining, along with focal context information. ATHENATEST achieves significant improvements, with pretraining yielding a 25\% relative improvement and focal context contributing an additional 11.1\% improvement in terms of validation loss. The paper introduces METHODS2TEST, a substantial publicly available corpus of unit test case methods and corresponding focal methods in Java. ATHENATEST is evaluated on five defects4j projects, generating around 25,000 passing test cases covering 43.7\% of focal methods with only 30 attempts. Comparative analysis against EvoSuite and GPT-3 reveals ATHENATEST's superior performance, outperforming GPT-3 and exhibiting comparable coverage to EvoSuite. The study includes a developer survey, indicating overwhelming preferences for ATHENATEST-generated test cases over EvoSuite's, suggesting significant potential for improving developer productivity.

In the study titled "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation,"\cite{tang_chatgpt_2023} conducted by Tang et al., the authors explore the capabilities of ChatGPT in the context of unit test suite generation and compare its performance with Search-Based Software Testing (SBST). The research investigates key aspects, including correctness, readability, code coverage, and bug detection, shedding light on the strengths and limitations of both approaches. The findings reveal that ChatGPT generates Java test cases, with 69.6\% successfully compiling and executing. Despite a majority being bug-free, some test cases exhibit severe issues. The study evaluates the readability of ChatGPT-generated test suites by assessing code style violations, highlighting predominant concerns such as indentation and adherence to coding rules. In terms of code coverage, EvoSuite outperforms ChatGPT, suggesting potential scenarios where LLMs like ChatGPT might excel. The study emphasizes the effectiveness of EvoSuite in bug detection, attributing it to the genetic algorithm and feedback mechanisms, proposing a hybrid approach combining SBST and LLM for enhanced software testing accuracy. The limitations include restricted access to ChatGPT's internals and time-bound performance considerations. Overall, the research contributes valuable insights into the comparative assessment of LLMs and SBST in software engineering.

In the paper titled "ChatUniTest: a ChatGPT-based automated unit test generation tool," authored by Xie et al., the authors introduce ChatUniTest\cite{xie_chatunitest_2023}, the first ChatGPT-based automated unit test generation tool. Evaluating its quality, ChatUniTest demonstrates a 30.86\% pass rate and 29.98\% correct test cases, showcasing its ability to generate high-quality unit tests. In a comparative analysis against EvoSuite, AthenaTest, and A3Test, ChatUniTest consistently outperforms EvoSuite in branch and line coverage and surpasses both AthenaTest and A3Test in various metrics, highlighting its robustness and superiority. Examining the contributions of different components, ChatGPT-based repair emerges as a significant enhancer, playing a crucial role in improving test case generation. The study also addresses the cost-effectiveness of unit test generation, revealing repair costs as the primary contributor. Despite limitations such as API cost and limited access to ChatGPT's internals, ChatUniTest marks a significant advancement in the realm of automated unit test generation, offering valuable insights and promising results.

In the paper titled "No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation"\cite{yuan_no_2023}, Yuanet et al. present a comprehensive evaluation of ChatGPT's unit test generation capabilities. Through rigorous quantitative analysis and a user study, the authors address key research questions. Firstly, in terms of correctness, ChatGPT significantly outperforms existing learning-based techniques, showcasing its prowess in syntactic, compilation, and execution correctness. Despite facing diverse compilation errors, its generated tests exhibit comparable sufficiency and coverage to manually-written tests. The readability of ChatGPT-generated tests is found to be decent and comparable to manual tests. The tests also demonstrate high usability potential, with a considerable number of participants expressing willingness to adopt them. Additionally, the authors introduce CHATTESTER\cite{yuan_no_2023}, a novel technique that substantially improves the correctness of ChatGPT-generated tests by reducing compilation errors and incorrect assertions. CHATTESTER outperforms ChatGPT by generating 34.3\% more compilable tests and 18.7\% more tests with correct assertions. The findings highlight the limitations and prospects of ChatGPT-based unit test generation, offering practical insights for improvement. The study provides a valuable contribution to the evolving landscape of automated unit test generation techniques.