\chapter{Introduction}
\label{cha:introduction}
\vspace{0.4 cm}

In the ever-evolving landscape of software development, ensuring the reliability and robustness of applications is a continuous pursuit. Software testing stands as a cornerstone of quality assurance, a meticulous process where developers manually create tests—guiding benchmarks known as test oracles—to determine if software behaves as intended. However, manual test oracle generation poses challenges—it's resource-intensive, error-prone, and struggles to keep pace with the dynamic evolution of software systems. To address these challenges, various automated test generation techniques have emerged, yet they too face limitations. Tools like EvoSuite rely on code implementation, and when errors exist in the code, generated tests may misconstrue these errors as actual behavior, leading to the Oracle problem in software testing.

Enter Large Language Models (LLMs), an exciting frontier in artificial intelligence. Trained on extensive natural language data, LLMs exhibit a unique ability to comprehend instructions and understand code contexts. This raises a compelling question: Can LLMs leverage their linguistic and coding knowledge to enhance the oracles of automatically generated tests?

In response to this inquiry, our thesis introduces EvoOracle—a groundbreaking tool designed to explore the intersection of LLMs and software testing. EvoOracle takes Java projects, extracts critical information, replaces assertions in test cases with placeholders, and harnesses the power of LLMs to generate precise assertions. The key innovation lies in the potential of LLMs to understand the expected behavior of code, transcending the limitations of traditional automated test generation tools. We embark on a comprehensive comparison with the EvoSuite baseline, aiming to showcase the transformative capabilities of LLMs in automating test oracle creation.

This journey delves into the synergy between machine learning and software testing, pushing the boundaries of what's possible in enhancing software reliability. Through meticulous experimentation and evaluation, we seek to unravel the true potential of LLMs in redefining the future of software testing. Our work not only addresses the Oracle problem but also opens avenues for broader applications of LLMs in the realm of software engineering. In conclusion, this thesis embarks on a journey into the future of software testing, where Large Language Models emerge as intelligent allies in the pursuit of software reliability and innovation. Our work holds the potential to redefine software quality assurance, offering a glimpse into a world where artificial intelligence meets code-centric domains, promising a future where software correctness is achieved with unprecedented ease and precision. 

The thesis unfolds in a structured manner, offering an insightful journey into the realms of Large Language Models (LLMs) and their application in automated test oracle generation. In Chapter~\ref{cha:background}, we lay the foundation with a detailed exploration of LLMs, delving into their underlying concepts and significance. Simultaneously, we scrutinize the landscape of software testing and the crucial role of test oracles, setting the stage for the challenges addressed in this research. Additionally, we navigate through the landscape of automated test generation, identifying existing methodologies and their limitations. For readers already acquainted with these concepts, a smooth transition to Chapter~\ref{cha:state_of_the_art} is encouraged, as it builds upon this foundational knowledge.

Chapter~\ref{cha:state_of_the_art} unfurls as a comprehensive canvas with a methodology for identifying relevant papers, ensuring a robust literature review. This chapter meticulously examines the state of the art in oracle generation and unit test generation, providing a backdrop for the innovative approaches proposed in this thesis.

The core of our exploration unfolds in Chapter~\ref{cha:evoOracles}, where we delve into the practical implementation of LLMs for oracle generation. The journey begins with a detailed account of data collection from Java projects, incorporating the intricate process of test case generation using EvoSuite. Subsequently, we navigate through preprocessing steps, including the removal of assertions, placeholder insertion, and the intricate integration of LLMs into the testing workflow. Each step is methodically detailed, offering transparency into our experimental design.

Chapter~\ref{cha:experiments} serves as the crucible of experimentation, outlining the experimental setup, research questions, and the meticulous evaluation of results. We embark on a rigorous analysis of the effectiveness and efficiency of LLMs, answering critical research questions and addressing potential threats to validity. This chapter forms the empirical backbone, substantiating the transformative potential of LLM-driven test oracle generation.

In the concluding chapter, Chapter~\ref{cha:conclusions}, we distill the essence of our findings, summarizing key insights and their implications. We outline avenues for future research, shedding light on the broader implications of our work in the domain of software testing and beyond.