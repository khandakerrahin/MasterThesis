\chapter{Introduction}
\label{cha:introduction}
\vspace{0.4 cm}

In the ever-evolving landscape of software development, ensuring the reliability and robustness of applications is a continuous pursuit. Software testing stands as a cornerstone of quality assurance, a meticulous process where developers manually create tests—guiding benchmarks known as test oracles—to determine if software behaves as intended. However, manual test oracle generation poses challenges\cite{6963470}—it's resource-intensive, error-prone, and struggles to keep pace with the dynamic evolution of software systems. To address these challenges, various automated test generation techniques\cite{6915770} have emerged, yet they too face limitations. Tools like EvoSuite\cite{noauthor_evosuite_nodate} rely on code implementation, and when errors exist in the code, generated tests may misconstrue these errors as actual behavior, leading to the Oracle problem in software testing.

Enter Large Language Models (LLMs)\cite{petroni_language_2019}, an exciting frontier in artificial intelligence. Trained on extensive natural language data, LLMs exhibit a unique ability to comprehend instructions and understand code contexts. This raises a compelling question: Can LLMs leverage their linguistic and coding knowledge to enhance the oracles of automatically generated tests?

In response to this inquiry, this thesis introduces EvoOracle—a tool designed to explore the intersection of LLMs and software testing. EvoOracle takes Java projects, extracts critical information, replaces assertions in test cases with placeholders, and harnesses the power of LLMs to generate precise assertions. The key innovation lies in the potential of LLMs to understand the expected behavior of code, transcending the limitations of traditional automated test generation tools. We embark on a comprehensive comparison with the EvoSuite baseline, aiming to showcase the transformative capabilities of LLMs in automating test oracle creation.

This thesis focuses into the synergy between machine learning and software testing, pushing the boundaries of what's possible in enhancing software reliability. Through systematic experimentation and evaluation, we seek to unravel the true potential of LLMs in redefining the future of software testing. Our work not only addresses the Oracle problem but also opens avenues for broader applications of LLMs in the realm of software engineering. We explore the future of software testing, where Large Language Models emerge as intelligent allies in the pursuit of software reliability and innovation. Our work has the potential to contribute to the evolution of software quality assurance, exploring the intersection of artificial intelligence and code-centric domains. It envisions a future where achieving software correctness is facilitated with enhanced ease and precision.

The thesis unfolds in a structured manner, offering an insightful journey into the realms of Large Language Models (LLMs) and their application in automated test oracle generation. In Chapter~\ref{cha:background}, we lay the foundation with a detailed exploration of LLMs, delving into their underlying concepts and significance. Simultaneously, we scrutinize the landscape of software testing and the crucial role of test oracles, setting the stage for the challenges addressed in this research. Additionally, we navigate through the landscape of automated test generation, identifying existing methodologies and their limitations. For readers already acquainted with these concepts, a smooth transition to Chapter~\ref{cha:state_of_the_art} is encouraged, as it builds upon this foundational knowledge.

In Chapter~\ref{cha:state_of_the_art}, the methodology for identifying relevant papers are describe. This chapter also examines the state of the art in oracle generation and unit test generation, providing a backdrop for the innovative approaches proposed in this thesis.

In Chapter~\ref{cha:evoOracles}, our approach to solving challenges in software testing is discussed. We introduce EvoOracle, our tool that uses Large Language Models (LLMs) to streamline test oracle creation. 

Chapter~\ref{cha:experiments} serves as the crucible of experimentation, outlining the experimental setup, research questions, and the meticulous evaluation of results. The chapter begins with a detailed account of data collection from Java projects, incorporating the intricate process of test case generation using EvoSuite. Subsequently, we navigate through preprocessing steps, including the removal of assertions, placeholder insertion, and the intricate integration of LLMs into the testing workflow. Each step is methodically detailed, offering transparency into our experimental design.

We embark on a rigorous analysis of the effectiveness and efficiency of LLMs, answering critical research questions and addressing potential threats to validity. This chapter forms the empirical backbone, substantiating the transformative potential of LLM-driven test oracle generation.

In the concluding chapter, Chapter~\ref{cha:conclusions}, we distill the essence of our findings, summarizing key insights and their implications. We outline avenues for future research, shedding light on the broader implications of our work in the domain of software testing and beyond.