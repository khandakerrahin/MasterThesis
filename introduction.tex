\chapter{Introduction}
\label{cha:introduction}
\vspace{0.4 cm}

In the dynamic realm of software development, the quest for robust and reliable software applications is unending. Software testing, a vital component of quality assurance, relies heavily on the creation and maintenance of test oracles—benchmarks that determine whether software behaves as expected. However, manual test oracle generation is resource-intensive, error-prone, and often struggles to keep pace with the rapid evolution of software systems. This thesis ventures into a pioneering domain, investigating the potential of Large Language Models (LLMs) as a game-changing solution to the challenges of automated test oracle generation. Grounded in a Data Science perspective, we introduce a novel tool that seamlessly integrates LLMs into the software testing workflow. Our objective is twofold: to assess the feasibility of employing LLMs for test oracle generation and to evaluate the effectiveness of LLM-generated test oracles in enhancing software quality. Through a rigorous methodology encompassing Java project data extraction, LLM-based oracle generation, and extensive experimentation, we unveil the transformative capabilities of LLMs in automating test oracle creation. Our research reveals that LLMs are capable of understanding programming contexts and generate precise assertions, potentially revolutionizing the software testing landscape. The results of our experiments demonstrate not only the feasibility of LLM-driven test oracle generation but also the tangible improvements in test quality and efficiency achieved by this novel approach. We compare LLM-generated test oracles with traditional methods, shedding light on the advantages and challenges inherent in this paradigm shift. Beyond the confines of software testing, this research opens doors to broader applications of LLMs in software engineering, signaling a promising intersection of artificial intelligence and code-centric domains. We discuss the implications of our findings, addressing feasibility, effectiveness, and practical considerations. In conclusion, this thesis embarks on a journey into the future of software testing, where Large Language Models emerge as intelligent allies in the pursuit of software reliability and innovation. Our work holds the potential to redefine software quality assurance, offering a glimpse into a world where software correctness is achieved with unprecedented ease and precision.

The thesis is structured as follows. 
Chapter~\ref{cha:soa} presents a comprehensive literature review of the current state of the art, discussing key theories and previous studies related to the research questions.
The context of electricity data representation and time series forecasting methods are analyzed, including an extensive analysis of two prominent subjects in the research community such as Transformers and AutoML.
Furthermore, the three use cases of interest, electricity demand forecasting, consumption baseline forecasting, and electricity production forecasting, are treated in detail in dedicated sections.
In Chapter~\ref{cha:system}, the model of the proposed system is presented.
An in-depth presentation of the designed system architecture is provided and the different components of the system with their functionalities and interactions are described.
Moreover, the modules focused on the three use cases of interest are treated more in detail in dedicated sections.
Chapter~\ref{cha:implementation} presents how the prototype of the system was developed and how the key components of the designed architecture are implemented.
The implementation of the system’s common components across the various specific use cases is first presented and then dedicated sections explain the implementation details of the use case-specific modules.
In Chapter~\ref{cha:evaluation}, the system prototype is validated and the performance of the models for the different use cases is evaluated.
The datasets provided by MIWenergía and the adopted evaluation methodology are first described and then dedicated sections analyze and discuss the performance of the models for the different use cases.
Finally, Chapter~\ref{cha:conclusions} reports the conclusions of the thesis, summarizing the key findings, their implications, and suggesting avenues for future research.
