% book
% Required fields: author or editor, title, publisher, year. 
% Optional fields: volume or number, series, address, edition, month, note.

% article
% Required fields: author, title, journal, year. 
% Optional fields: volume, number, pages, month, note.

% conference
% same as inproceedings
% Required fields: author, title, booktitle, year. 
% Optional fields: editor, volume or number, series, pages, address, month, organization, publisher, note

% website
% as misc
% Required fields: none. 
% Optional fields: author, title, howpublished, month, year, note.


% 1. TOGA

@inproceedings{gabriel_ryan_toga_2022,
	title = {{TOGA}: A Neural Method for Test Oracle Generation},
	volume = {2022-May},
        url = {https://dl.acm.org/doi/abs/10.1145/3510003.3510141},
	doi = {10.1145/3510003.3510141},
	abstract = {Testing is widely recognized as an important stage of the software development lifecycle. Effective software testing can provide benefits such as bug finding, preventing regressions, and documentation. In terms of documentation, unit tests express a unit's intended functionality, as conceived by the developer. A test oracle, typically expressed as an condition, documents the intended behavior of a unit under a given test prefix. Synthesizing a functional test oracle is a challenging problem, as it must capture the intended functionality rather than the implemented functionality. In this paper, we propose {TOGA} (a neural method for Test Oracle {GenerAtion}), a unified transformer-based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method. Our approach can handle units with ambiguous or missing documentation, and even units with a missing implementation. We evaluate our approach on both oracle inference accuracy and functional bug-finding. Our technique improves accuracy by 33\% over existing oracle inference approaches, achieving 96\% over-all accuracy on a held out test dataset. Furthermore, we show that when integrated with a automated test generation tool ({EvoSuite}), our approach finds 57 real world bugs in large-scale Java programs, including 30 bugs that are not found by any other automated testing method in our evaluation. © 2022 {ACM}.},
	pages = {2130 -- 2141},
	booktitle = {Proceedings - International Conference on Software Engineering},
	author = {{Gabriel Ryan} and {Todd Mytkowicz} and {Shuvendu K. Lahiri} and {Elizabeth Dinella}},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Software testing, Learning systems, Machine learning, Machine-learning, Software design, Program debugging, Statistical tests, Software testings, Electric transformer testing, Bug finding, Condition, Functional test, Life cycle, Software development life-cycle, Test oracles, Transformer, Unit tests},
	annotation = {Cited by: 2; All Open Access, Green Open Access},
	file = {Submitted Version:/home/shaker/Zotero/storage/525P4QRC/Dinella et al. - 2022 - TOGA A Neural Method for Test Oracle Generation.pdf:application/pdf},
}

@inproceedings{tufano_generating_2022,
	location = {New York, {NY}, {USA}},
	title = {Generating accurate assert statements for unit test cases using pretrained transformers},
	isbn = {978-1-4503-9286-0},
	url = {https://dl.acm.org/doi/10.1145/3524481.3527220},
	doi = {10.1145/3524481.3527220},
	series = {{AST} '22},
	abstract = {Unit testing represents the foundational basis of the software testing pyramid, beneath integration and end-to-end testing. Automated software testing researchers have proposed a variety of techniques to assist developers in this time-consuming task. In this paper we present an approach to support developers in writing unit test cases by generating accurate and useful assert statements. Our approach is based on a state-of-the-art transformer model initially pretrained on an English textual corpus. This semantically rich model is then trained in a semi-supervised fashion on a large corpus of source code. Finally, we finetune this model on the task of generating assert statements for unit tests. The resulting model is able to generate accurate assert statements for a given method under test. In our empirical evaluation, the model was able to predict the exact assert statements written by developers in 62\% of the cases in the first attempt. The results show 80\% relative improvement for top-1 accuracy over the previous {RNN}-based approach in the literature, as well as 33\% improvement over the recent Transformer-based T5 approach. We also show the substantial impact of the pretraining process on the performances of our model, as well as comparing it with assert auto-completion task. Finally, we demonstrate how our approach can be used to augment {EvoSuite} test cases, with additional asserts leading to improved test coverage.},
	pages = {54--64},
	booktitle = {Proceedings of the 3rd {ACM}/{IEEE} International Conference on Automation of Software Test},
	publisher = {Association for Computing Machinery},
	author = {Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Sundaresan, Neel},
	urldate = {2023-06-08},
	date = {2022-07-19},
	keywords = {neural networks, software testing, unit test},
	file = {Full Text PDF:/home/shaker/Zotero/storage/DR79V28J/Tufano et al. - 2022 - Generating accurate assert statements for unit tes.pdf:application/pdf},
}

@misc{nie_learning_2023,
	title = {Learning Deep Semantics for Test Completion},
	url = {http://arxiv.org/abs/2302.10166},
	abstract = {Writing tests is a time-consuming yet essential task during software development. We propose to leverage recent advances in deep learning for text and code generation to assist developers in writing tests. We formalize the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test. We develop {TeCo} -- a deep learning model using code semantics for test completion. The key insight underlying {TeCo} is that predicting the next statement in a test method requires reasoning about code execution, which is hard to do with only syntax-level data that existing code completion models use. {TeCo} extracts and uses six kinds of code semantics data, including the execution result of prior statements and the execution context of the test method. To provide a testbed for this new task, as well as to evaluate {TeCo}, we collect a corpus of 130,934 test methods from 1,270 open-source Java projects. Our results show that {TeCo} achieves an exact-match accuracy of 18, which is 29\% higher than the best baseline using syntax-level data only. When measuring functional correctness of generated next statement, {TeCo} can generate runnable code in 29\% of the cases compared to 18\% obtained by the best baseline. Moreover, {TeCo} is significantly better than prior work on test oracle generation.},
	number = {{arXiv}:2302.10166},
	publisher = {{arXiv}},
	author = {Nie, Pengyu and Banerjee, Rahul and Li, Junyi Jessy and Mooney, Raymond J. and Gligoric, Milos},
	urldate = {2023-06-09},
	date = {2023-03-07},
	eprinttype = {arxiv},
	eprint = {2302.10166 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annotation = {Comment: Accepted as a conference paper in {ICSE} 2023},
	file = {arXiv.org Snapshot:/home/shaker/Zotero/storage/HYZ3FLC8/2302.html:text/html;Full Text PDF:/home/shaker/Zotero/storage/8STUDGVP/Nie et al. - 2023 - Learning Deep Semantics for Test Completion.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2023-10-01},
	date = {2017},
	file = {Full Text PDF:/home/shaker/Zotero/storage/VLVDK8ZJ/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}


@inproceedings{petroni_language_2019,
	title = {Language models as knowledge bases?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080640117&partnerID=40&md5=b1f9f407f6ebe93f1b83f1cb65225251},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream {NLP} tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, {BERT} contains relational knowledge competitive with traditional {NLP} methods that have some access to oracle knowledge, (ii) {BERT} also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain {QA} systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/{LAMA}. © 2019 Association for Computational Linguistics},
	pages = {2463 -- 2473},
	booktitle = {{EMNLP}-{IJCNLP} 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	date = {2019},
	note = {Type: Conference paper},
	keywords = {State of the art, Computational linguistics, Search engines, Factual knowledge, Human supervision, In-depth analysis, Linguistic knowledge, Natural language processing systems, Open domain question answering, Query processing, Structured knowledge, Without fine-tuning},
	annotation = {Cited by: 569},
}

@article{zakeri_nasrabadi_format-aware_2021,
	title = {Format-aware learn\&fuzz: deep test data generation for efficient fuzzing},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086442107&doi=10.1007%2fs00521-020-05039-7&partnerID=40&md5=d8c3129c57c345a68e73fcd61502ae77},
	doi = {10.1007/s00521-020-05039-7},
	abstract = {Appropriate test data are a crucial factor to succeed in fuzz testing. Most of the real-world applications, however, accept complex structure inputs containing data surrounded by meta-data which is processed in several stages comprising of the parsing and rendering (execution). The complex structure of some input files makes it difficult to generate efficient test data automatically. The success of deep learning to cope with complex tasks, specifically generative tasks, has motivated us to exploit it in the context of test data generation for complicated structures such as {PDF} files. In this respect, a neural language model ({NLM}) based on deep recurrent neural networks ({RNNs}) is used to learn the structure of complex inputs. To target both the parsing and rendering steps of the software under test ({SUT}), our approach generates new test data while distinguishing between data and meta-data that significantly improve the input fuzzing. To assess the proposed approach, we have developed a modular file format fuzzer, {IUST}-{DeepFuzz}. Our experimental results demonstrate the relatively high coverage of {MuPDF} code by our proposed fuzzer, {IUST}-{DeepFuzz}, in comparison with the state-of-the-art tools such as learn\&fuzz, {AFL}, Augmented-{AFL}, and random fuzzing. In summary, our experiments with many deep learning models revealed the fact that the simpler the deep learning models applied to generate test data, the higher the code coverage of the software under test will be. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
	pages = {1497 -- 1513},
	number = {5},
	journaltitle = {Neural Computing and Applications},
	author = {Zakeri Nasrabadi, Morteza and Parsa, Saeed and Kalaee, Akram},
	date = {2021},
	note = {Type: Article},
	keywords = {Language model, State of the art, Codes (symbols), Software testing, Recurrent neural networks, Complex inputs, Complex networks, Complex structure, Complicated structures, Deep neural networks, Learning models, Metadata, Recurrent neural network ({RNNs}), Test data generation, Testing},
	annotation = {Cited by: 6; All Open Access, Green Open Access},
}

@inproceedings{khanfir_effective_2022,
	title = {Effective and scalable fault injection using bug reports and generative language models},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143077167&doi=10.1145%2f3540250.3558907&partnerID=40&md5=2e5e92a5ba88a26873fb385216db5e36},
	doi = {10.1145/3540250.3558907},
	abstract = {Previous research has shown that artificial faults can be useful in many software engineering tasks such as testing, fault-tolerance assessment, debugging, dependability evaluation, risk analysis, etc. However, such artificial-fault-based applications can be questioned or inaccurate when the considered faults misrepresent real bugs. Since typically, fault injection techniques (i.e. mutation testing) produce a large number of faults by altering "blindly"the code in arbitrary locations, they are unlikely capable to produce few but relevant real-like faults. In our work, we tackle this challenge by guiding the injection towards resembling bugs that have been previously introduced by developers. For this purpose, we propose {iBiR}, the first fault injection approach that leverages information from bug reports to inject "realistic"faults. {iBiR} injects faults on the locations that are more likely to be related to a given bug-report by applying appropriate inverted fix-patterns, which are manually or automatically crafted by automated-program-repair researchers. We assess our approach using bugs from the Defects4J dataset and show that {iBiR} outperforms significantly conventional mutation testing in terms of injecting faults that semantically resemble and couple with real ones, in the vast majority of the cases. Similarly, the faults produced by {iBiR} give significantly better fault-tolerance estimates than conventional mutation testing in around 80\% of the cases. © 2022 Owner/Author.},
	pages = {1790 -- 1794},
	booktitle = {{ESEC}/{FSE} 2022 - Proceedings of the 30th {ACM} Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	author = {Khanfir, Ahmed},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Computational linguistics, Software testing, Application programs, Bug reports, Dependability evaluation, Engineering tasks, Fault injection, Fault Injection techniques, Fault tolerance, Fault-based, Mutation, Mutation testing, Naturalness, Program debugging, Risk analysis, Risk assessment, Statistical tests},
	annotation = {Cited by: 0; All Open Access, Bronze Open Access},
}

@inproceedings{pham_application_2022,
	title = {Application of Natural Language Processing Towards Autonomous Software Testing},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146929359&doi=10.1145%2f3551349.3563241&partnerID=40&md5=fb509df27839612062ef0fc8f58e8b8e},
	doi = {10.1145/3551349.3563241},
	abstract = {The process of creating test cases from requirements written in natural language ({NL}) requires intensive human efforts and can be tedious, repetitive, and error-prone. Thus, many studies have attempted to automate that process by utilizing Natural Language Processing ({NLP}) approaches. Furthermore, with the advent of massive language models and transfer learning techniques, people have introduced various advancements in {NLP}-assisted software testing with promising results. More notably, in recent years, not only have researchers been engrossed in solving the above task, but many companies have also embedded the feature to translate from human language to test cases their products. This paper presents an overview of {NLP}-assisted solutions being used in both the literature and the software testing industry. © 2022 {ACM}.},
	booktitle = {{ACM} International Conference Proceeding Series},
	author = {Pham, Khang and Nguyen, Vu and Nguyen, Tien},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Natural languages, Language model, Software testing, Learning systems, Natural language processing systems, Language processing, Application programs, Autonomous software, Error prones, Model learning, Processing approach, Software testings, Test case, Transfer learning},
	annotation = {Cited by: 0; All Open Access, Bronze Open Access},
	file = {Pham et al. - 2022 - Application of Natural Language Processing Towards.pdf:/home/shaker/Zotero/storage/ELE9W9BS/Pham et al. - 2022 - Application of Natural Language Processing Towards.pdf:application/pdf},
}

@inproceedings{xu_dsmith_2020,
	title = {{DSmith}: Compiler Fuzzing through Generative Deep Learning Model with Attention},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093834400&doi=10.1109%2fIJCNN48605.2020.9206911&partnerID=40&md5=42b69bf257fa35fbbd9100dfe800758a},
	doi = {10.1109/IJCNN48605.2020.9206911},
	abstract = {Compiler fuzzing is a technique to test the functionalities of compiler. It requires well-formed test cases (i.e., programs) that have correct lexicons and syntax to pass the parsing stage of a compiler. Recently, advanced compiler fuzzing methods generate effective test cases by deep neural networks, which learn the language model of regular programs to guarantee test case quality. However, most of these methods fail to capture long-distance dependencies of syntax (e.g., paired curly braces) in a program. As a result, they may generate test cases with syntax errors, which cannot pass the parsing stage to test the compiler functionality. In this paper, we propose a framework, namely {DSmith}, to capture long-distance dependencies of syntax for a robust test case generation. Specifically, {DSmith} memorizes the hidden state of each token in a program and leverages the interactions of these hidden states to embed the long-distance dependencies between tokens. It then adopts an encoder-decoder architecture with the embedding of these long-distance dependencies to build a language model of regular programs. Finally, {DSmith} uses the built language model to generate test cases according to four novel generation strategies, which significantly increase the diversity of test cases. Extensive experiments show that {DSmith} increases the parsing pass rate of the generated programs by an average of 19\% and significantly improves the code coverage of the compiler, compared with state-of-the-art methods. Benefiting from the high pass rate and broad code coverage, {DSmith} has found eleven brand new bugs in currently supported {GCC} compiler versions. © 2020 {IEEE}.},
	booktitle = {Proceedings of the International Joint Conference on Neural Networks},
	author = {Xu, Haoran and Wang, Yongjun and Fan, Shuhui and Xie, Peidai and Liu, Aizhi},
	date = {2020},
	note = {Type: Conference paper},
	keywords = {Neural networks, Language model, Codes (symbols), Computational linguistics, Software testing, Deep learning, Deep neural networks, Learning models, Code coverage, Encoder-decoder architecture, Long-distance dependencies, Program compilers, Robust tests, State-of-the-art methods, Syntactics, Syntax errors},
	annotation = {Cited by: 6},
}

@inproceedings{bellan_experiment_2022,
	title = {Experiment Maker: a Tool to create Experiments with {GPT}-3 easily},
	volume = {3256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142424840&partnerID=40&md5=1bf71eaa7524dc628e7117e79ba697ec},
	abstract = {Novel large pre-trained language models, such as {GPT}-3, can be considered and adopted as artificial agents since they are now able to solve general problems and mimic human experts. The introduction of the in-context learning technique allows interaction with the model directly by instructing it to solve a task. Task instructions, the actual input data, and optionally some examples of solutions are packed together in a single prompt. The model interprets the prompt and generates a solution for the given problem without any need of fine-tuning the model. However, designing efficient prompts is more an art than a science, nowadays. When starting from scratch, to achieve good performance different prompt contents and model engine's configurations (for the same prompt) must be tested. These can be considered time-intensive operations. In this paper, we present Experiment Maker a software developed to save time and minimize the effort in designing and testing different prompts and different configurations, In addition, the tool supports users in combining multiple prompts into an experimental pipeline. Experiment Maker can be downloaded from the project page at github.com/patriziobellan86/{ExperimentMaker}. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International ({CC} {BY} 4.0)},
	booktitle = {{CEUR} Workshop Proceedings},
	author = {Bellan, Patrizio and Dragoni, Mauro and Ghidini, Chiara},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Software testing, Learning systems, High level languages, Artificial agents, Context learning, Creative Commons, {GPT}-3, Human expert, In contexts, In-context learning, Input datas, Learning techniques, Python},
	annotation = {Cited by: 0},
}

@inproceedings{ye_pt-fuzz_2022,
	title = {{PT}-Fuzz: A Transformer Based Fuzzing Data Generation Method},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153362785&doi=10.1145%2f3579895.3579937&partnerID=40&md5=7e8dae3d523139a50e5cce3cb2d61dd6},
	doi = {10.1145/3579895.3579937},
	abstract = {American Fuzzy Lop ({AFL}) is one of the most widely used overlay-oriented fuzzers in the field of fuzz processing. In the process of analyzing {AFL}, we found that the number of tests per code block in the {AFL} testing process is very unevenly distributed. The {AFL} spends a lot of testing time on a small number of code blocks. In addition, the fuzzing cannot discover new paths for a long time because new basic blocks cannot be discovered in time. These two reasons lead to a waste of fuzz testing performance. In this paper, we propose a test case generation method that combines path information and deep learning. The deep learning model is used to analyze the relationship between the program execution path and test cases. In addition, the deep learning model also learns the syntactic rules of program input to generate better test cases. We implemented our approach based on the {AFL} and Transformer model. And test the effect of our method in a real program. Experimental results show that our method can improve the efficiency of fuzzing. On average, {PT}-Fuzz found 14.4\% more paths and 7.2\% more code blocks than {AFL}. © 2022 {ACM}.},
	pages = {277 -- 283},
	booktitle = {{ACM} International Conference Proceeding Series},
	author = {Ye, Yingyun and Zhuang, Yan},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Codes (symbols), Software testing, Deep learning, Learning systems, Learning models, Test case, Code blocks, Data generation, Fuzzing, Generation method, Testing process, Testing time, Vulnerabilities minings},
	annotation = {Cited by: 0},
}

@inproceedings{richter_learning_2022,
	title = {Learning Realistic Mutations: Bug Creation for Neural Bug Detectors},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133242895&doi=10.1109%2fICST53961.2022.00027&partnerID=40&md5=b81af38c4f6d35b000d0c3a13f1f3f0d},
	doi = {10.1109/ICST53961.2022.00027},
	abstract = {Mutations are small, often token-level changes to program code, typically performed during mutation testing for evaluating the quality of test suites. Recently, code mutations have come in use for creating benchmarks of buggy code. Such bug benchmarks present valuable aids for the evaluation of testing, debugging or bug repair tools. Moreover, they can serve as training data for learning-based (neural) bug detectors. Key to all these applications is the creation of realistic bugs which closely resemble mistakes made by software developers. In this paper, we present a learning-based approach to mutation. We propose a novel contextual mutation operator which incorporates knowledge about the mutation context to inject natural and more realistic bugs into code. Our approach employs a masked language model to produce a context-dependent distribution over feasible token replacements. The strategy for producing realistic mutations is thus learned. Our experimental evaluation on Java, {JavaScript} and Python programs shows that sampling from a language model does not only produce mutants which more accurately represent real bugs (with a reproduction score nearly 70\% higher than for mutations employed in testing), but also lead to better performing bug detectors when trained on thus generated bug benchmarks. © 2022 {IEEE}.},
	pages = {162 -- 173},
	booktitle = {Proceedings - 2022 {IEEE} 15th International Conference on Software Testing, Verification and Validation, {ICST} 2022},
	author = {Richter, Cedric and Wehrheim, Heike},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Computational linguistics, Software testing, Application programs, Mutation, Mutation testing, Program debugging, Python, Bug creation, Bug detection, Bug detector, Cell proliferation, Code mutation, Level change, Natural code, Program code},
	annotation = {Cited by: 2},
}

@inproceedings{muradoglu_eeny_2022,
	title = {Eeny, meeny, miny, moe. How to choose data for morphological inflection},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149436217&partnerID=40&md5=773f332cc4fd051d463bd22af3171efc},
	abstract = {Data scarcity is a widespread problem in numerous natural language processing ({NLP}) tasks for low-resource languages. Within morphology, the labour-intensive work of tagging/glossing data is a serious bottleneck for both {NLP} and language documentation. Active learning ({AL}) aims to reduce the cost of data annotation by selecting data that is most informative for improving the model. In this paper, we explore four sampling strategies for the task of morphological inflection using a Transformer model: a pair of oracle experiments where data is chosen based on whether the model already can or cannot inflect the test forms correctly, as well as strategies based on high/low model confidence, entropy, as well as random selection. We investigate the robustness of each strategy across 30 typologically diverse languages. We also perform a more in-depth case study of Natügu. Our results show a clear benefit to selecting data based on model confidence and entropy. Unsurprisingly, the oracle experiment, where only incorrectly handled forms are chosen for further training, which is presented as a proxy for linguist/language consultant feedback, shows the most improvement. This is followed closely by choosing low-confidence and high-entropy predictions. We also show that despite the conventional wisdom of larger data sets yielding better accuracy, introducing more instances of high-confidence or low-entropy forms, or forms that the model can already inflect correctly, can reduce model performance. © 2022 Association for Computational Linguistics.},
	pages = {7294 -- 7303},
	booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2022},
	author = {Muradoglu, Saliha and Hulden, Mans},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Natural languages, Natural language processing systems, Language processing, Transformer modeling, Active Learning, Low resource languages, Data annotation, Data scarcity, Electric transformer testing, Entropy, High-low, Labour-intensive, Sampling strategies},
	annotation = {Cited by: 0},
}

@article{wei_cocofuzzing_2022,
	title = {{CoCoFuzzing}: Testing Neural {\textless}underline{\textgreater}Co{\textless}/underline{\textgreater}de Models With {\textless}underline{\textgreater}Co{\textless}/underline{\textgreater}verage-Guided {\textless}underline{\textgreater}Fuzzing{\textless}/underline{\textgreater}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139828771&doi=10.1109%2fTR.2022.3208239&partnerID=40&md5=5071551c554cd6b591e535fb21e91f35},
	doi = {10.1109/TR.2022.3208239},
	abstract = {Deep learning ({DL})-based code processing models have demonstrated good performance for tasks such as method name prediction, program summarization, and comment generation. However, despite the tremendous advancements, {DL} models are frequently susceptible to adversarial attacks, which pose a significant threat to the robustness and generalizability of these models by causing them to misclassify unexpected inputs. To address the issue above, numerous {DL} testing approaches have been proposed; however, these approaches primarily target testing {DL} applications in the domains of image, audio, and text analysis, etc., and cannot be \&\#x201C;directly applied\&\#x201D; to \&\#x201C;neural models for code\&\#x201D; due to the unique properties of programs. In this article, we propose a coverage-based fuzzing framework, {\textless}monospace{\textgreater}{CoCoFuzzing}{\textless}/monospace{\textgreater}, for testing {DL}-based code processing models. In particular, we first propose 10 mutation operators to automatically generate validly and semantically preserving source code examples as tests, followed by a neuron coverage ({NC})-based approach for guiding the generation of tests. The performance of {\textless}monospace{\textgreater}{CoCoFuzzing}{\textless}/monospace{\textgreater} is evaluated using three state-of-the-art neural code models, i.e., {NeuralCodeSum}, {CODE}2SEQ, and {CODE}2VEC. Our experiment results indicate that {\textless}monospace{\textgreater}{CoCoFuzzing}{\textless}/monospace{\textgreater} can generate validly and semantically preserving source code examples for testing the robustness and generalizability of these models and enhancing {NC}. Furthermore, these tests can be used for adversarial retraining to improve the performance of neural code models. {IEEE}},
	pages = {1--14},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Wei, Moshi and Huang, Yuchao and Yang, Jinqiu and Wang, Junjie and Wang, Song},
	date = {2022},
	note = {Type: Article},
	keywords = {Language model, Codes (symbols), Computer programming languages, Software testing, Deep learning, Application programs, Fuzzing, Biological neural networks, Code, Code model, Fuzzy logic, Fuzzy neural networks, Fuzzy-Logic, Job analysis, Network coding, Robustness, Software, Task analysis},
	annotation = {Cited by: 0; All Open Access, Green Open Access},
}

@inproceedings{li_software_2022,
	title = {Software Vulnerability Detection Based on Anomaly-Attention},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143629307&doi=10.1109%2fICRCV55858.2022.9953210&partnerID=40&md5=384292bee2f8b4a760db456f2cb50dba},
	doi = {10.1109/ICRCV55858.2022.9953210},
	abstract = {Software vulnerability is a decisive factor affecting the security of information systems. There are problems in vulnerability detection such as traditional static methods cannot cope with the detection scenarios of software without source code, and dynamic methods consume many resources are inefficient. Aiming at the potential vulnerabilities that may occur in the operation of executable programs without source code, this paper proposes a Transformer software vulnerability detection method based on Anomaly-Attention, which introduces the Anomaly-Attention mechanism in the traditional Transformer model to calculate the association differences between function call nodes to neighboring points and function call nodes to the whole sequence respectively, and then distinguish normal function call points from abnormal function call points through the association differences. The {CallSeq} tool is used to collect the sequence of function calls in program operation as timing data, and the fuzzy test results are used as labels to train the model in this paper. Experiments show that the method in this paper can detect vulnerabilities in unsourced executable programs with 87.73\% accuracy, which is about 3\% higher than the vulnerability detection accuracy of methods based on {CNN}, {LSTM} and other network models. © 2022 {IEEE}.},
	pages = {261 -- 265},
	booktitle = {2022 4th International Conference on Robotics and Computer Vision, {ICRCV} 2022},
	author = {Li, Shanshan and Chen, Deng and Zhang, Jun and Wang, Haoyu and Li, Lei and Qian, Yuyang and Liu, Hailun},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Computer programming languages, Software testing, Network security, Long short-term memory, Transformer, Source codes, Correlation difference, Exception attention, Executable programs, Function calls, Security of information system, Software vulnerabilities, Software without source code, Vulnerability detection},
	annotation = {Cited by: 0},
}

@inproceedings{pauzi_extracting_2021,
	title = {Extracting and Comparing Concepts Emerging from Software Code, Documentation and Tests},
	volume = {3071},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123481087&partnerID=40&md5=5dee8a933c0e235ca2a7c21cfd0868ef},
	abstract = {Traceability in software engineering is the ability to connect different artifacts that have been built or designed at various points in time. Given the variety of tasks, tools and formats in the software lifecycle, an outstanding challenge for traceability studies is to deal with the heterogeneity of the artifacts, the links between them and the means to extract each. Using a unified approach for extracting keywords from textual information, this paper aims to compare the concepts extracted from three software artifacts: source code, documentation and tests from the same system. The objectives are to detect similarities in the concepts emerged, and to show the degree of alignment and synchronisation the artifacts possess. Using the components of three projects from the Apache Software Foundation, this paper extracts the concepts from 'base' source code, documentation, and tests (separated from the source code). The extraction is done based on the keywords present in each artifact: we then run multiple comparisons (through calculating cosine similarities on features extracted by word embeddings) in order to detect how the sets of concepts are similar or overlap. For similarities between code and tests, we discovered that using pre-trained language models (with increasing dimension and corpus size) correlates to the increase in magnitude, with higher averages and smaller ranges. {FastText} pre-trained embeddings scored the highest average of 97.33\% with the lowest range of 21.8 across all projects. Also, our approach was able to quickly detect outliers, possibly indicating drifts in traceability within modules. For similarities involving documentation, there was a considerable drop in similarity score compared to between code and tests per module - down to below 5\%. © 2021 {CEUR}-{WS}. All rights reserved.},
	booktitle = {{CEUR} Workshop Proceedings},
	author = {Pauzi, Zaki and Capiluppi, Andrea},
	date = {2021},
	note = {Type: Conference paper},
	keywords = {Computer programming languages, Software testing, Natural language processing systems, Testing, Textual information, Embeddings, Life cycle, Source codes, Degree of alignments, Software artefacts, Software codes, Software life cycles, Software traceability, Textual-analysis, Unified approach},
	annotation = {Cited by: 1},
}

@inproceedings{zlotchevski_exploring_2022,
	title = {Exploring and evaluating personalized models for code generation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143074080&doi=10.1145%2f3540250.3558959&partnerID=40&md5=463752d113d0e6a1f7fa5672af633a69},
	doi = {10.1145/3540250.3558959},
	abstract = {Large Transformer models achieved the state-of-the-art status for Natural Language Understanding tasks and are increasingly becoming the baseline model architecture for modeling source code. Transformers are usually pre-trained on large unsupervised corpora, learning token representations and transformations relevant to modeling generally available text, and are then fine-tuned on a particular downstream task of interest. While fine-tuning is a tried-and-true method for adapting a model to a new domain - for example, question-answering on a given topic - generalization remains an on-going challenge. In this paper, we explore and evaluate transformer model fine-tuning for personalization. In the context of generating unit tests for Java methods, we evaluate learning to personalize to a specific software project using several personalization techniques. We consider three key approaches: (i) custom fine-tuning, which allows all the model parameters to be tuned; (ii) lightweight fine-tuning, which freezes most of the model's parameters, allowing tuning of the token embeddings and softmax layer only or the final layer alone; (iii) prefix tuning, which keeps model parameters frozen, but optimizes a small project-specific prefix vector. Each of these techniques offers a trade-off in total compute cost and predictive performance, which we evaluate by code and task-specific metrics, training time, and total computational operations. We compare these fine-tuning strategies for code generation and discuss the potential generalization and cost benefits of each in various deployment scenarios. © 2022 {ACM}.},
	pages = {1500 -- 1508},
	booktitle = {{ESEC}/{FSE} 2022 - Proceedings of the 30th {ACM} Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	author = {Zlotchevski, Andrei and Drain, Dawn and Svyatkovskiy, Alexey and Clement, Colin B. and Sundaresan, Neel and Tufano, Michele},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {State of the art, Codes (symbols), Software testing, Modeling languages, Learning systems, Natural language processing systems, Transformer modeling, Fine tuning, Economic and social effects, Baseline models, Codegeneration, Generalisation, Modeling parameters, Natural language understanding, Personalizations, Personalized model},
	annotation = {Cited by: 0; All Open Access, Green Open Access},
}

@inproceedings{degiovanni_bert_2022,
	title = {μBert: Mutation Testing using Pre-Trained Language Models},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133293969&doi=10.1109%2fICSTW55395.2022.00039&partnerID=40&md5=dc46365bd72bc1d5fc7ededdf7c1631d},
	doi = {10.1109/ICSTW55395.2022.00039},
	abstract = {We introduce μBert, a mutation testing tool that uses a pre-trained language model ({CodeBERT}) to generate mutants. This is done by masking a token from the expression given as input and using {CodeBERT} to predict it. Thus, the mutants are generated by replacing the masked tokens with the predicted ones. We evaluate μBert on 40 real faults from Defects4J and show that it can detect 27 out of the 40 faults, while the baseline ({PiTest}) detects 26 of them. We also show that μBert can be 2 times more cost-effective than {PiTest}, when the same number of mutants are analysed. Additionally, we evaluate the impact of μBert's mutants when used by program assertion inference techniques, and show that they can help in producing better specifications. Finally, we discuss about the quality and naturalness of some interesting mutants produced by μBert during our experimental evaluation. © 2022 {IEEE}.},
	pages = {160 -- 169},
	booktitle = {Proceedings - 2022 {IEEE} 14th International Conference on Software Testing, Verification and Validation Workshops, {ICSTW} 2022},
	author = {Degiovanni, Renzo and Papadakis, Mike},
	date = {2022},
	note = {Type: Conference paper},
	keywords = {Natural languages, Language model, Computational linguistics, Software testing, Natural language processing systems, Language processing, Mutation testing, Natural language processing, Quality control, {CodeBERT}, Cost effective, Cost effectiveness, Fault seeding, Inference techniques, Testing tools},
	annotation = {Cited by: 0; All Open Access, Green Open Access},
}

@inproceedings{liu_deepsqli_2020,
	title = {{DeepSQLi}: Deep semantic learning for testing {SQL} injection},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088916703&doi=10.1145%2f3395363.3397375&partnerID=40&md5=f1929e57a3f0596ef7ebc6c0cfda4f45},
	doi = {10.1145/3395363.3397375},
	abstract = {Security is unarguably the most serious concern for Web applications, to which {SQL} injection ({SQLi}) attack is one of the most devastating attacks. Automatically testing {SQLi} vulnerabilities is of ultimate importance, yet is unfortunately far from trivial to implement. This is because the existence of a huge, or potentially infinite, number of variants and semantic possibilities of {SQL} leading to {SQLi} attacks on various Web applications. In this paper, we propose a deep natural language processing based tool, dubbed {DeepSQLi}, to generate test cases for detecting {SQLi} vulnerabilities. Through adopting deep learning based neural language model and sequence of words prediction, {DeepSQLi} is equipped with the ability to learn the semantic knowledge embedded in {SQLi} attacks, allowing it to translate user inputs (or a test case) into a new test case, which is se- mantically related and potentially more sophisticated. Experiments are conducted to compare {DeepSQLi} with {SQLmap}, a state-of-the-art {SQLi} testing automation tool, on six real-world Web applications that are of different scales, characteristics and domains. Empirical results demonstrate the effectiveness and the remarkable superiority of {DeepSQLi} over {SQLmap}, such that more {SQLi} vulnerabilities can be identified by using a less number of test cases, whilst running much faster. © 2020 {ACM}.},
	pages = {286 -- 297},
	booktitle = {{ISSTA} 2020 - Proceedings of the 29th {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	author = {Liu, Muyang and Li, Ke and Chen, Tao},
	date = {2020},
	note = {Type: Conference paper},
	keywords = {Language model, State of the art, Software testing, Deep learning, Natural language processing systems, {NAtural} language processing, {WEB} application, Real world web, Semantic knowledge, Semantic learning, Testing automation},
	annotation = {Cited by: 11; All Open Access, Green Open Access},
}

@article{almeida_automatic_2018,
	title = {Automatic test case generation for concurrent features from natural language descriptions},
	volume = {11254 {LNCS}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057283362&doi=10.1007%2f978-3-030-03044-5_11&partnerID=40&md5=5c198ff0c76eef14e0568f3c3efbbc61},
	doi = {10.1007/978-3-030-03044-5_11},
	abstract = {Contemporary computing applications have an increasing level of concurrency; new techniques are demanded to tackle the challenge of testing the plentiful interactions that arise from concurrent behaviour. Current approaches for automatic test generation from natural language models do not allow the explicit specification of concurrent behaviour. This paper extends our previous test case generation approach to support concurrent mobile device features. A natural language notation is proposed to express the composition of sequential and concurrent behaviour. The notation can be automatically translated to a {CSP} model, from which tests are automatically produced using the {FDR} refinement checker. The approach is illustrated with a mobile application that includes concurrent features. © Springer Nature Switzerland {AG} 2018.},
	pages = {163 -- 179},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Almeida, Rafaela and Nogueira, Sidney and Sampaio, Augusto},
	date = {2018},
	note = {Type: Conference paper},
	keywords = {Software testing, Test case generation, Computer operating procedures, Mobile applications, Natural language model, Formal methods, Automatic test generation, Automatic test-case generations, Computing applications, Concurrent feature, {FDR} refinement checker},
	annotation = {Cited by: 0},
}

@article{fatima_flakify_2023,
	title = {Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137568682&doi=10.1109%2fTSE.2022.3201209&partnerID=40&md5=72a0b07c369ed41812cf679e98002f8a},
	doi = {10.1109/TSE.2022.3201209},
	abstract = {Software testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing and failing across executions, even for the same version of the source code. Flaky test cases introduce overhead to software development as they can lead to unnecessary attempts to debug production or testing code. Besides rerunning test cases multiple times, which is time-consuming and computationally expensive, flaky test cases can be predicted using machine learning ({ML}) models, thus reducing the wasted cost of re-running and debugging these test cases. However, the state-of-the-art {ML}-based flaky test case predictors rely on pre-defined sets of features that are either project-specific, i.e., inapplicable to other projects, or require access to production code, which is not always available to software test engineers. Moreover, given the non-deterministic behavior of flaky test cases, it can be challenging to determine a complete set of features that could potentially be associated with test flakiness. Therefore, in this article, we propose Flakify, a black-box, language model-based predictor for flaky test cases. Flakify relies exclusively on the source code of test cases, thus not requiring to (a) access to production code (black-box), (b) rerun test cases, (c) pre-define features. To this end, we employed {CodeBERT}, a pre-trained language model, and fine-tuned it to predict flaky test cases using the source code of test cases. We evaluated Flakify on two publicly available datasets ({FlakeFlagger} and {IDoFT}) for flaky test cases and compared our technique with the {FlakeFlagger} approach, the best state-of-the-art {ML}-based, white-box predictor for flaky test cases, using two different evaluation procedures: (1) cross-validation and (2) per-project validation, i.e., prediction on new projects. Flakify achieved F1-scores of 79\% and 73\% on the {FlakeFlagger} dataset using cross-validation and per-project validation, respectively. Similarly, Flakify achieved F1-scores of 98\% and 89\% on the {IDoFT} dataset using the two validation procedures, respectively. Further, Flakify surpassed {FlakeFlagger} by 10 and 18 percentage points (pp) in terms of precision and recall, respectively, when evaluated on the {FlakeFlagger} dataset, thus reducing the cost bound to be wasted on unnecessarily debugging test cases and production code by the same percentages (corresponding to reduction rates of 25\% and 64\%). Flakify also achieved significantly higher prediction results when used to predict test cases on new projects, suggesting better generalizability over {FlakeFlagger}. Our results further show that a black-box version of {FlakeFlagger} is not a viable option for predicting flaky test cases. © 1976-2012 {IEEE}.},
	pages = {1912 -- 1927},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Fatima, Sakina and Ghaleb, Taher A. and Briand, Lionel},
	date = {2023},
	note = {Type: Article},
	keywords = {Natural languages, Predictive models, Codes (symbols), Computational linguistics, Modeling languages, Learning systems, Software design, Natural language processing systems, Language processing, Testing, Program debugging, Software testings, Natural language processing, C (programming language), Features extraction, Code, Software, Learning algorithms, Object oriented programming, {CodeBERT}, Black-box testing, Computational modelling, Feature extraction, Flaky test, Forecasting},
	annotation = {Cited by: 1; All Open Access, Green Open Access},
}

@misc{xie_chatunitest_2023,
	title = {{ChatUniTest}: a {ChatGPT}-based automated unit test generation tool},
	url = {http://arxiv.org/abs/2305.04764},
	shorttitle = {{ChatUniTest}},
	abstract = {Unit testing is a crucial, yet often tedious and time-consuming task. To relieve developers from this burden, automated unit test generation techniques are developed. Existing automated unit test generation tools, such as program-analysis-based tools like {EvoSuite} and Randoop, lack program comprehension, resulting in unit tests with poor readability and limited assertions. Language-model-based tools, such as {AthenaTest} and A3Test, have limitations in the generation of correct unit tests. In this paper, we introduce {ChatUniTest}, a {ChatGPT}-based automated unit test generation tool developed under the Generation-Validation-Repair framework. {ChatUniTest} generates tests by parsing the project, extracting essential information, and creating an adaptive focal context that includes the focal method and its dependencies within the pre-defined maximum prompt token limit. The context is incorporated into a prompt and subsequently submitted to {ChatGPT}. Once {ChatGPT}'s response is received, {ChatUniTest} proceeds to extract the raw test from the response. It then validates the test and employs rule-based repair to fix syntactic and simple compile errors, followed by {ChatGPT}-based repair to address challenging errors. Our rigorous evaluation demonstrates that {ChatUniTest} outperforms {EvoSuite} in branch and line coverage, surpasses {AthenaTest} and A3Test in focal method coverage, and effectively generates assertions while utilizing mock objects and reflection to achieve test objectives.},
	number = {{arXiv}:2305.04764},
	publisher = {{arXiv}},
	author = {Xie, Zhuokui and Chen, Yinghao and Zhi, Chen and Deng, Shuiguang and Yin, Jianwei},
	urldate = {2023-05-19},
	date = {2023-05-08},
	eprinttype = {arxiv},
	eprint = {2305.04764 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/home/shaker/Zotero/storage/2M7HPBF6/2305.html:text/html;Full Text PDF:/home/shaker/Zotero/storage/MITBTLLB/Xie et al. - 2023 - ChatUniTest a ChatGPT-based automated unit test g.pdf:application/pdf},
}

@misc{yuan_no_2023,
	title = {No More Manual Tests? Evaluating and Improving {ChatGPT} for Unit Test Generation},
	url = {http://arxiv.org/abs/2305.04207},
	shorttitle = {No More Manual Tests?},
	abstract = {Unit testing is essential in detecting bugs in functionally-discrete program units. Manually writing high-quality unit tests is time-consuming and laborious. Although traditional techniques can generate tests with reasonable coverage, they exhibit low readability and cannot be directly adopted by developers. Recent work has shown the large potential of large language models ({LLMs}) in unit test generation, which can generate more human-like and meaningful test code. {ChatGPT}, the latest {LLM} incorporating instruction tuning and reinforcement learning, has performed well in various domains. However, It remains unclear how effective {ChatGPT} is in unit test generation. In this work, we perform the first empirical study to evaluate {ChatGPT}'s capability of unit test generation. Specifically, we conduct a quantitative analysis and a user study to systematically investigate the quality of its generated tests regarding the correctness, sufficiency, readability, and usability. The tests generated by {ChatGPT} still suffer from correctness issues, including diverse compilation errors and execution failures. Still, the passing tests generated by {ChatGPT} resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with {ChatGPT} could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we propose {ChatTESTER}, a novel {ChatGPT}-based unit test generation approach, which leverages {ChatGPT} itself to improve the quality of its generated tests. {ChatTESTER} incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of {ChatTESTER} by generating 34.3\% more compilable tests and 18.7\% more tests with correct assertions than the default {ChatGPT}.},
	number = {{arXiv}:2305.04207},
	publisher = {{arXiv}},
	author = {Yuan, Zhiqiang and Lou, Yiling and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin},
	urldate = {2023-05-19},
	date = {2023-05-08},
	eprinttype = {arxiv},
	eprint = {2305.04207 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/home/shaker/Zotero/storage/UPFDLV8Z/2305.html:text/html;Full Text PDF:/home/shaker/Zotero/storage/TM9ALG86/Yuan et al. - 2023 - No More Manual Tests Evaluating and Improving Cha.pdf:application/pdf},
}

@misc{tufano_unit_2021,
	title = {Unit Test Case Generation with Transformers and Focal Context},
	url = {http://arxiv.org/abs/2009.05617},
	doi = {10.48550/arXiv.2009.05617},
	abstract = {Automated unit test case generation tools facilitate test-driven development and support developers by suggesting tests intended to identify flaws in their code. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult for developers to read or understand. In this paper we propose {AthenaTest}, an approach that aims to generate unit test cases by learning from real-world focal methods and developer-written testcases. We formulate unit test case generation as a sequence-to-sequence learning task, adopting a two-step training procedure consisting of denoising pretraining on a large unsupervised Java corpus, and supervised finetuning for a downstream translation task of generating unit tests. We investigate the impact of natural language and source code pretraining, as well as the focal context information surrounding the focal method. Both techniques provide improvements in terms of validation loss, with pretraining yielding 25\% relative improvement and focal context providing additional 11.1\% improvement. We also introduce Methods2Test, the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 780K test cases mined from 91K open-source repositories from {GitHub}. We evaluate {AthenaTest} on five defects4j projects, generating 25K passing test cases covering 43.7\% of the focal methods with only 30 attempts. We execute the test cases, collect test coverage information, and compare them with test cases generated by {EvoSuite} and {GPT}-3, finding that our approach outperforms {GPT}-3 and has comparable coverage w.r.t. {EvoSuite}. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated tests, showing overwhelmingly preference towards {AthenaTest}.},
	number = {{arXiv}:2009.05617},
	publisher = {{arXiv}},
	author = {Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Deng, Shao Kun and Sundaresan, Neel},
	urldate = {2023-06-09},
	date = {2021-05-20},
	eprinttype = {arxiv},
	eprint = {2009.05617 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/shaker/Zotero/storage/V82T8YSH/Tufano et al. - 2021 - Unit Test Case Generation with Transformers and Fo.pdf:application/pdf;arXiv.org Snapshot:/home/shaker/Zotero/storage/MIWCG7YQ/2009.html:text/html},
}

@misc{bareis_code_2022,
	title = {Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code},
	url = {http://arxiv.org/abs/2206.01335},
	doi = {10.48550/arXiv.2206.01335},
	shorttitle = {Code Generation Tools (Almost) for Free?},
	abstract = {Few-shot learning with large-scale, pre-trained language models is a powerful way to answer questions about code, e.g., how to complete a given code example, or even generate code snippets from scratch. The success of these models raises the question whether they could serve as a basis for building a wide range code generation tools. Traditionally, such tools are built manually and separately for each task. Instead, few-shot learning may allow to obtain different tools from a single pre-trained language model by simply providing a few examples or a natural language description of the expected tool behavior. This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose. We consider three code manipulation and code generation tasks targeted by a range of traditional tools: (i) code mutation; (ii) test oracle generation from natural language documentation; and (iii) test case generation. For each task, we compare few-shot learning to a manually built tool. Our results show that the model-based tools complement (code mutation), are on par (test oracle generation), or even outperform their respective traditionally built tool (test case generation), while imposing far less effort to develop them. By comparing the effectiveness of different variants of the model-based tools, we provide insights on how to design an appropriate input ("prompt") to the model and what influence the size of the model has. For example, we find that providing a small natural language description of the code generation task is an easy way to improve predictions. Overall, we conclude that few-shot language models are surprisingly effective, yet there is still more work to be done, such as exploring more diverse ways of prompting and tackling even more involved tasks.},
	number = {{arXiv}:2206.01335},
	publisher = {{arXiv}},
	author = {Bareiß, Patrick and Souza, Beatriz and d'Amorim, Marcelo and Pradel, Michael},
	urldate = {2023-06-09},
	date = {2022-06-12},
	eprinttype = {arxiv},
	eprint = {2206.01335 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	annotation = {Comment: 12 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/shaker/Zotero/storage/S97KHBVW/Bareiß et al. - 2022 - Code Generation Tools (Almost) for Free A Study o.pdf:application/pdf;arXiv.org Snapshot:/home/shaker/Zotero/storage/UAR53NLT/2206.html:text/html},
}

@inproceedings{yu_automated_2022,
	location = {New York, {NY}, {USA}},
	title = {Automated assertion generation via information retrieval and its integration with deep learning},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510149},
	doi = {10.1145/3510003.3510149},
	series = {{ICSE} '22},
	abstract = {Unit testing could be used to validate the correctness of basic units of the software system under test. To reduce manual efforts in conducting unit testing, the research community has contributed with tools that automatically generate unit test cases, including test inputs and test oracles (e.g., assertions). Recently, {ATLAS}, a deep learning ({DL}) based approach, was proposed to generate assertions for a unit test based on other already written unit tests. Despite promising, the effectiveness of {ATLAS} is still limited. To improve the effectiveness, in this work, we make the first attempt to leverage Information Retrieval ({IR}) in assertion generation and propose an {IR}-based approach, including the technique of {IR}-based assertion retrieval and the technique of retrieved-assertion adaptation. In addition, we propose an integration approach to combine our {IR}-based approach with a {DL}-based approach (e.g., {ATLAS}) to further improve the effectiveness. Our experimental results show that our {IR}-based approach outperforms the state-of-the-art {DL}-based approach, and integrating our {IR}-based approach with the {DL}-based approach can further achieve higher accuracy. Our results convey an important message that information retrieval could be competitive and worthwhile to pursue for software engineering tasks such as assertion generation, and should be seriously considered by the research community given that in recent years deep learning solutions have been over-popularly adopted by the research community for software engineering tasks.},
	pages = {163--174},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Yu, Hao and Lou, Yiling and Sun, Ke and Ran, Dezhi and Xie, Tao and Hao, Dan and Li, Ying and Li, Ge and Wang, Qianxiang},
	urldate = {2023-06-08},
	date = {2022-07-05},
	keywords = {deep learning, information retrieval, test assertion, unit testing},
	file = {Full Text PDF:/home/shaker/Zotero/storage/XJ26XGCU/Yu et al. - 2022 - Automated assertion generation via information ret.pdf:application/pdf},
}

@misc{schafer_adaptive_2023,
	title = {Adaptive Test Generation Using a Large Language Model},
	url = {http://arxiv.org/abs/2302.06527},
	doi = {10.48550/arXiv.2302.06527},
	abstract = {Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. This paper presents {TestPilot}, an adaptive test generation technique that leverages Large Language Models ({LLMs}). {TestPilot} uses Codex, an off-the-shelf {LLM}, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests. In our approach, Codex is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. If a generated test fails, {TestPilot}'s adaptive component attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We created an implementation of {TestPilot} for {JavaScript} and evaluated it on 25 npm packages with a total of 1,684 {API} functions to generate tests for. Our results show that the generated tests achieve up to 93.1\% statement coverage (median 68.2\%). Moreover, on average, 58.5\% of the generated tests contain at least one assertion that exercises functionality from the package under test. Our experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. Finally, we find that {TestPilot} does not generate memorized tests: 92.7\% of our generated tests have \${\textbackslash}leq\$ 50\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies.},
	number = {{arXiv}:2302.06527},
	publisher = {{arXiv}},
	author = {Schäfer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
	urldate = {2023-06-09},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2302.06527 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/shaker/Zotero/storage/3CEVK8Q4/Schäfer et al. - 2023 - Adaptive Test Generation Using a Large Language Mo.pdf:application/pdf;arXiv.org Snapshot:/home/shaker/Zotero/storage/RBKBD866/2302.html:text/html},
}

@misc{liu_towards_2023,
	title = {Towards More Realistic Evaluation for Neural Test Oracle Generation},
	url = {http://arxiv.org/abs/2305.17047},
	doi = {10.48550/arXiv.2305.17047},
	abstract = {Effective unit tests can help guard and improve software quality but require a substantial amount of time and effort to write and maintain. A unit test consists of a test prefix and a test oracle. Synthesizing test oracles, especially functional oracles, is a well-known challenging problem. Recent studies proposed to leverage neural models to generate test oracles, i.e., neural test oracle generation ({NTOG}), and obtained promising results. However, after a systematic inspection, we find there are some inappropriate settings in existing evaluation methods for {NTOG}. These settings could mislead the understanding of existing {NTOG} approaches' performance. We summarize them as 1) generating test prefixes from bug-fixed program versions, 2) evaluating with an unrealistic metric, and 3) lacking a straightforward baseline. In this paper, we first investigate the impacts of these settings on evaluating and understanding the performance of {NTOG} approaches. We find that 1) unrealistically generating test prefixes from bug-fixed program versions inflates the number of bugs found by the state-of-the-art {NTOG} approach {TOGA} by 61.8\%, 2) {FPR} (False Positive Rate) is not a realistic evaluation metric and the Precision of {TOGA} is only 0.38\%, and 3) a straightforward baseline {NoException}, which simply expects no exception should be raised, can find 61\% of the bugs found by {TOGA} with twice the Precision. Furthermore, we introduce an additional ranking step to existing evaluation methods and propose an evaluation metric named Found@K to better measure the cost-effectiveness of {NTOG} approaches. We propose a novel unsupervised ranking method to instantiate this ranking step, significantly improving the cost-effectiveness of {TOGA}. Eventually, we propose a more realistic evaluation method {TEval}+ for {NTOG} and summarize seven rules of thumb to boost {NTOG} approaches into their practical usages.},
	number = {{arXiv}:2305.17047},
	publisher = {{arXiv}},
	author = {Liu, Zhongxin and Liu, Kui and Xia, Xin and Yang, Xiaohu},
	urldate = {2023-06-09},
	date = {2023-05-26},
	eprinttype = {arxiv},
	eprint = {2305.17047 [cs]},
	keywords = {Computer Science - Software Engineering},
	annotation = {Comment: Accepted by {ISSTA} 2023},
	file = {arXiv Fulltext PDF:/home/shaker/Zotero/storage/PRPUDQQE/Liu et al. - 2023 - Towards More Realistic Evaluation for Neural Test .pdf:application/pdf;arXiv.org Snapshot:/home/shaker/Zotero/storage/CD9RXS3F/2305.html:text/html},
}

@misc{tang_chatgpt_2023,
	title = {{ChatGPT} vs {SBST}: A Comparative Assessment of Unit Test Suite Generation},
	url = {http://arxiv.org/abs/2307.00588},
	shorttitle = {{ChatGPT} vs {SBST}},
	abstract = {Recent advancements in large language models ({LLMs}) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, {LLMs} have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the {ChatGPT} {LLM} and the state-of-the-art {SBST} tool {EvoSuite}. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of {LLMs} (specifically {ChatGPT}) in generating unit test cases compared to {EvoSuite}, this work provides valuable insights into the performance of {LLMs} in solving software engineering problems. Overall, our findings underscore the potential of {LLMs} in software engineering and pave the way for further research in this area.},
	number = {{arXiv}:2307.00588},
	publisher = {{arXiv}},
	author = {Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu},
	urldate = {2023-07-07},
	date = {2023-07-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.00588 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Tang et al. - 2023 - ChatGPT vs SBST A Comparative Assessment of Unit .pdf:/home/shaker/Zotero/storage/JCAFB5H6/Tang et al. - 2023 - ChatGPT vs SBST A Comparative Assessment of Unit .pdf:application/pdf},
}

@inproceedings{afshan_evolving_2013,
	title = {Evolving Readable String Test Inputs Using a Natural Language Model to Reduce Human Oracle Cost},
	doi = {10.1109/ICST.2013.11},
	abstract = {The frequent non-availability of an automated oracle means that, in practice, checking software behaviour is frequently a painstakingly manual task. Despite the high cost of human oracle involvement, there has been little research investigating how to make the role easier and less time-consuming. One source of human oracle cost is the inherent unreadability of machine-generated test inputs. In particular, automatically generated string inputs tend to be arbitrary sequences of characters that are awkward to read. This makes test cases hard to comprehend and time-consuming to check. In this paper we present an approach in which a natural language model is incorporated into a search-based input data generation process with the aim of improving the human readability of generated strings. We further present a human study of test inputs generated using the technique on 17 open source Java case studies. For 10 of the case studies, the participants recorded significantly faster times when evaluating inputs produced using the language model, with medium to large effect sizes 60\% of the time. In addition, the study found that accuracy of test input evaluation was also significantly improved for 3 of the case studies.},
	eventtitle = {Verification and Validation 2013 {IEEE} Sixth International Conference on Software Testing},
	pages = {352--361},
	booktitle = {Verification and Validation 2013 {IEEE} Sixth International Conference on Software Testing},
	author = {Afshan, Sheeva and {McMinn}, Phil and Stevenson, Mark},
	date = {2013-03},
	note = {{ISSN}: 2159-4848},
	keywords = {Natural languages, Java, Testing, Software, Strings, Computational modeling, Crowd-Sourced Human Study, Data models, Generators, Language Model, Search-Based Testing},
	file = {IEEE Xplore Abstract Record:/home/shaker/Zotero/storage/YAYQBH4R/stamp.html:text/html;IEEE Xplore Full Text PDF:/home/shaker/Zotero/storage/3T8ULVVK/Afshan et al. - 2013 - Evolving Readable String Test Inputs Using a Natur.pdf:application/pdf},
}

@inproceedings{lemieux_codamosa_2023,
	location = {Melbourne, Australia},
	title = {{CodaMosa}: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models},
	isbn = {978-1-66545-701-9},
	url = {https://ieeexplore.ieee.org/document/10172800/},
	doi = {10.1109/ICSE48619.2023.00085},
	shorttitle = {{CodaMosa}},
	abstract = {Search-based software testing ({SBST}) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. {SBST}’s performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, {SBST} can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models ({LLMs}) of code, such as {OpenAI}’s Codex, can be used to help {SBST}’s exploration. Our proposed algorithm, {CODAMOSA}, conducts {SBST} until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help {SBST} redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, {CODAMOSA} achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to {SBST} and {LLM}-only baselines.},
	eventtitle = {2023 {IEEE}/{ACM} 45th International Conference on Software Engineering ({ICSE})},
	pages = {919--931},
	booktitle = {2023 {IEEE}/{ACM} 45th International Conference on Software Engineering ({ICSE})},
	publisher = {{IEEE}},
	author = {Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha},
	urldate = {2023-10-03},
	date = {2023-05},
	langid = {english},
	file = {Lemieux et al. - 2023 - CodaMosa Escaping Coverage Plateaus in Test Gener.pdf:/home/shaker/Zotero/storage/NASNLCF3/Lemieux et al. - 2023 - CodaMosa Escaping Coverage Plateaus in Test Gener.pdf:application/pdf},
}


@software{sletteberg_frontend-maven-plugin_2023,
	title = {frontend-maven-plugin},
	rights = {Apache-2.0},
	url = {https://github.com/eirslett/frontend-maven-plugin},
	abstract = {"Maven-node-grunt-gulp-npm-node-plugin to end all maven-node-grunt-gulp-npm-plugins." A Maven plugin that downloads/installs Node and {NPM} locally, runs {NPM} install, Grunt, Gulp and/or Karma.},
	author = {Sletteberg, Eirik},
	urldate = {2023-10-04},
	date = {2023-09-28},
	note = {original-date: 2013-08-30T14:44:38Z},
}

@online{noauthor_bytedecojavacv_nodate,
	title = {bytedeco/javacv: Java interface to {OpenCV}, {FFmpeg}, and more},
	url = {https://github.com/bytedeco/javacv},
	urldate = {2023-10-04},
	file = {bytedeco/javacv\: Java interface to OpenCV, FFmpeg, and more:/home/shaker/Zotero/storage/GF7RWP2V/javacv.html:text/html},
}

@online{noauthor_bonigarciawebdrivermanager_nodate,
	title = {bonigarcia/webdrivermanager: Automated driver management and other helper features for Selenium {WebDriver} in Java},
	url = {https://github.com/bonigarcia/webdrivermanager},
	urldate = {2023-10-04},
	file = {bonigarcia/webdrivermanager\: Automated driver management and other helper features for Selenium WebDriver in Java:/home/shaker/Zotero/storage/HZBW7L6W/webdrivermanager.html:text/html},
}

@online{noauthor_authorjappszerocode_nodate,
	title = {authorjapps/zerocode: A community-developed, free, opensource, automated testing framework for microservices {API}, Kafka and load testing built using {JUnit} core runners. Zerocode Open Source enables you to create, change, orchestrate and maintain your automated test scenarios declaratively with absolute ease},
	url = {https://github.com/authorjapps/zerocode},
	urldate = {2023-10-04},
}

@online{noauthor_evosuite_nodate,
	title = {{EvoSuite} {\textbar} Automatic Test Suite Generation for Java},
	url = {https://www.evosuite.org/},
	urldate = {2023-10-04},
}

@online{noauthor_pit_nodate,
	title = {{PIT} Mutation Testing},
	url = {https://pitest.org/},
	urldate = {2023-10-04},
	file = {PIT Mutation Testing:/home/shaker/Zotero/storage/GR7NTRGY/pitest.org.html:text/html},
}

@online{noauthor_randoop_nodate,
	title = {Randoop: Automatic unit test generation for Java},
	url = {https://randoop.github.io/randoop/},
	urldate = {2023-10-04},
	file = {Randoop\: Automatic unit test generation for Java:/home/shaker/Zotero/storage/F8YL3QW2/randoop.html:text/html},
}

@online{noauthor_tree-sitterintroduction_nodate,
	title = {Tree-sitter},
	url = {https://tree-sitter.github.io/tree-sitter/},
	urldate = {2023-10-05},
	file = {Tree-sitter-Introduction:/home/shaker/Zotero/storage/2WK4YG2S/tree-sitter.html:text/html},
}

@online{noauthor_assert_nodate,
	title = {Assert ({JUnit} {API})},
	url = {https://junit.org/junit4/javadoc/4.8/org/junit/Assert.html},
	urldate = {2023-10-05},
	file = {Assert (JUnit API):/home/shaker/Zotero/storage/992AA9G8/Assert.html:text/html},
}

@software{noauthor_unit_2023,
	title = {Unit Test Generation Task},
	rights = {{MIT}},
	url = {https://github.com/microsoft/methods2test},
	abstract = {methods2test is a supervised dataset consisting of Test Cases and their corresponding Focal Methods from a set of Java software repositories},
	publisher = {Microsoft},
	urldate = {2023-10-06},
	date = {2023-09-30},
	note = {original-date: 2020-08-05T05:13:29Z},
	keywords = {automated-testing, machine-learning},
}

@online{noauthor_gpt4all_nodate,
	title = {{GPT}4All},
	url = {https://www.gpt4all.io},
	abstract = {Free, local and privacy-aware chatbots},
	urldate = {2023-10-06},
	langid = {english},
	file = {Snapshot:/home/shaker/Zotero/storage/3K3NI2R9/index.html:text/html},
}

@misc{han_deep_2016,
	title = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
	url = {http://arxiv.org/abs/1510.00149},
	shorttitle = {Deep Compression},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difﬁcult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce “deep compression”, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method ﬁrst prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, ﬁnally, we apply Huffman coding. After the ﬁrst two steps we retrain the network to ﬁne tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the {ImageNet} dataset, our method reduced the storage required by {AlexNet} by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of {VGG}-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows ﬁtting the model into on-chip {SRAM} cache rather than off-chip {DRAM} memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on {CPU}, {GPU} and mobile {GPU}, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efﬁciency.},
	number = {{arXiv}:1510.00149},
	publisher = {{arXiv}},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	urldate = {2023-10-06},
	date = {2016-02-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1510.00149 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annotation = {Comment: Published as a conference paper at {ICLR} 2016 (oral)},
	file = {Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf:/home/shaker/Zotero/storage/JVGEGM67/Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf:application/pdf},
}

@online{noauthor_intel_nodate,
	title = {Intel® Xeon® Gold 6226R Processor (22M Cache, 2.90 {GHz}) - Product Specifications},
	url = {https://www.intel.com/content/www/us/en/products/sku/199347/intel-xeon-gold-6226r-processor-22m-cache-2-90-ghz/specifications.html},
	abstract = {Intel® Xeon® Gold 6226R Processor (22M Cache, 2.90 {GHz}) quick reference with specifications, features, and technologies.},
	titleaddon = {Intel},
	urldate = {2023-10-07},
	langid = {english},
	file = {Snapshot:/home/shaker/Zotero/storage/ZQPTPCKJ/specifications.html:text/html},
}

@online{noauthor_introduction_nodate,
	title = {Introduction to Singularity — Singularity container 3.5 documentation},
	url = {https://docs.sylabs.io/guides/3.5/user-guide/introduction.html},
	urldate = {2023-10-07},
	file = {Introduction to Singularity — Singularity container 3.5 documentation:/home/shaker/Zotero/storage/NJSWUQV7/introduction.html:text/html},
}


@misc{open-llm-leaderboard,
  author = {Edward Beeching},
  title = {Open LLM Leaderboard},
  year = {2023},
  publisher = {Hugging Face},
  howpublished = "\url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}"
}
@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}
@misc{clark2018think,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{zellers2019hellaswag,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?},
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding},
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@misc{lin2022truthfulqa,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{MosaicML2023Introducing,
    author    = {MosaicML NLP Team},
    title     = {Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-7b},
    note      = {Accessed: 2023-09-28},
    urldate   = {2023-09-28}
}

@online{noauthor_mosaicmlmpt-7b-chat_2023,
	title = {mosaicml/mpt-7b-chat · Hugging Face},
	url = {https://huggingface.co/mosaicml/mpt-7b-chat},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-10-07},
	date = {2023-05-26},
	file = {Snapshot:/home/shaker/Zotero/storage/TSV24SZ4/mpt-7b-chat.html:text/html},
}

@online{noauthor_nousresearchnous-hermes-13b_nodate,
	title = {{NousResearch}/Nous-Hermes-13b · Hugging Face},
	url = {https://huggingface.co/NousResearch/Nous-Hermes-13b},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-10-07},
	file = {Snapshot:/home/shaker/Zotero/storage/MGGI6SKC/Nous-Hermes-13b.html:text/html},
}

@online{noauthor_theblokewizardlm-13b-v1-1-superhot-8k-fp16_nodate,
	title = {{TheBloke}/{WizardLM}-13B-V1-1-{SuperHOT}-8K-fp16 · Hugging Face},
	url = {https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-fp16},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-10-07},
	file = {Snapshot:/home/shaker/Zotero/storage/LAZDLLL8/WizardLM-13B-V1-1-SuperHOT-8K-fp16.html:text/html},
}

@online{noauthor_theblokestable-vicuna-13b-hf_2023,
	title = {{TheBloke}/stable-vicuna-13B-{HF} · Hugging Face},
	url = {https://huggingface.co/TheBloke/stable-vicuna-13B-HF},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-10-07},
	date = {2023-05-22},
	file = {Snapshot:/home/shaker/Zotero/storage/3SAGE7NX/stable-vicuna-13B-HF.html:text/html},
}

@online{noauthor_pankajmathurorca_mini_v3_13b_2023,
	title = {pankajmathur/orca\_mini\_v3\_13b · Hugging Face},
	url = {https://huggingface.co/pankajmathur/orca_mini_v3_13b},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-10-07},
	date = {2023-09-25},
	file = {Snapshot:/home/shaker/Zotero/storage/YSKNZUSG/orca_mini_v3_13b.html:text/html},
}

@misc{mukherjee2023orca,
      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, 
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://vicuna.lmsys.org},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@online{noauthor_hugging_2023,
	title = {Hugging Face – The {AI} community building the future.},
	url = {https://huggingface.co/},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-10-07},
	date = {2023-10-05},
	file = {Snapshot:/home/shaker/Zotero/storage/A5PFZLG5/huggingface.co.html:text/html},
}

@online{noauthor_jinja_nodate,
	title = {Introduction — Jinja Documentation (3.1.x)},
	url = {https://jinja.palletsprojects.com/en/3.1.x/intro/},
	urldate = {2023-10-08},
	file = {Introduction — Jinja Documentation (3.1.x):/home/shaker/Zotero/storage/SKWPP6JH/intro.html:text/html},
}

@online{evooracle_github,
	title = {khandakerrahin/evooracle},
	url = {https://github.com/khandakerrahin/evooracle},
	urldate = {2023-10-08},
}

@online{evooracle_gitlab,
	title = {Shaker Mahmud Khandaker / {EvoOracle} · {GitLab}},
	url = {https://gitlab.fbk.eu/skhandaker/evooracle},
	abstract = {{GitLab} Enterprise Edition},
	titleaddon = {{GitLab}},
	urldate = {2023-10-08},
	date = {2023-10-01},
	langid = {english},
	file = {Snapshot:/home/shaker/Zotero/storage/M4VGNKXY/evooracle.html:text/html},
}

@inproceedings{svyatkovskiy_pythia_2019,
	location = {New York, {NY}, {USA}},
	title = {Pythia: {AI}-assisted Code Completion System},
	isbn = {978-1-4503-6201-6},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330699},
	doi = {10.1145/3292500.3330699},
	series = {{KDD} '19},
	shorttitle = {Pythia},
	abstract = {In this paper, we propose a novel end-to-end approach for {AI}-assisted code completion called Pythia. It generates ranked lists of method and {API} recommendations which can be used by software developers at edit time. The system is currently deployed as part of Intellicode extension in Visual Studio Code {IDE}. Pythia exploits state-of-the-art large-scale deep learning models trained on code contexts extracted from abstract syntax trees. It is designed to work at a high throughput predicting the best matching code completions on the order of 100 ms. We describe the architecture of the system, perform comparisons to frequency-based approach and invocation-based Markov Chain language model, and discuss challenges serving Pythia models on lightweight client devices. The offline evaluation results obtained on 2700 Python open source software {GitHub} repositories show a top-5 accuracy of 92\%, surpassing the baseline models by 20\% averaged over classes, for both intra and cross-project settings.},
	pages = {2727--2735},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel},
	urldate = {2023-10-08},
	date = {2019-07-25},
	keywords = {code completion, naturalness of software, neural networks},
	file = {Full Text PDF:/home/shaker/Zotero/storage/FPC3TIA2/Svyatkovskiy et al. - 2019 - Pythia AI-assisted Code Completion System.pdf:application/pdf},
}