\chapter{Conclusions and Future Work}
\label{cha:conclusions}
\vspace{0.4 cm}

\section{Conclusions}
\label{sec:conclusions}
\vspace{0.2 cm}

In the final chapters of this thesis, we explore the world of Large Language Models (LLMs) and their potential in automating test oracle generation. This journey unfolds by understanding LLMs and their crucial role in software testing. Simultaneously, we delve into the significance of test oracles in ensuring software integrity. As we progress, we face challenges in combining LLMs and test oracles, shaping our research path. Methodically, we navigate through automated test generation methods, examining their strengths and limitations.

Our research's heartbeat is a series of experiments, carefully designed to test and validate our proposed approaches. We analyze data from various Java projects on GitHub, uncovering insights. Challenges arise, such as dealing with randomness in LLM outputs and addressing biases from datasets. We balance generalizability with domain specificity while upholding the study's validity.

Beyond the experiments, our study creates a cohesive narrative, combining implications, contributions, and future paths. It's not just an empirical inquiry; it's a guide for practitioners, researchers, and industry players to understand the potential and challenges of integrating LLMs into automated test oracle generation. The study goes beyond answering questions; it offers methodological innovations, insights into LLM-generated oracles, and a call for a holistic view of language models and software testing. As we conclude this inquiry, it's not an endpoint but an invitationâ€”to explore further, understand deeper, and shape the future of software testing with Large Language Models.

The thesis addressed the following research questions:

\begin{itemize}
    \item \textbf{RQ1:} How is the quality of the EvoOracle generated Oracles?
    \item \textbf{RQ2:} How does the performance of EvoOracle generated assertions compare to assertions generated by traditional automated test generation tool, EvoSuite?
    \item \textbf{RQ3:} How does the performance of EvoSuite change if they are complemented with EvoOracle generated assertions?
    % \item \textbf{RQ4:} How many attempts are typically needed for successful assertion generation, and does the success rate improve with additional attempts following initial failure?
\end{itemize}

The main results of the thesis can be summarized as follows:
\begin{itemize}
  \item In addressing RQ1 for EvoOracle-generated Oracles across five models, we observed variable performance. MPT-7B-Chat faced challenges with SyntaxError (16.13\%) and CompileError (24.34\%). Orca\_mini\_13B-GGML\_v3 showed higher SyntaxError (29.08\%) but lower CompileError (4.17\%). StableVicuna-13B had zero SyntaxErrors but struggled with a high CompileError (63.58\%). WizardLM-13B-V1.1 had moderate SyntaxError (16.96\%) and CompileError (22.82\%). Nous-hermes-13B\_ggml\_v3 exhibited decent results, with SyntaxError (13.79\%) and CompileError (22.92\%). Overall, SyntaxError averaged 17.85\%, CompileError 22.05\%, and RuntimeError 2.18\%. The "Passed" rate was 9.26\%. These insights highlight areas for EvoOracle refinement in automated testing scenarios.
  \item In addressing RQ2, our mutation analysis shows EvoSuite consistently exhibits commendable mutation scores across all cases, underscoring its efficacy in mutant detection. LLMs, while not boosting performance, demonstrate nuanced impacts, emphasizing the class-specific nature of interactions. Nous-hermes-13B and Stable Vicuna 13B show better overall mutation coverage among the LLMs. Regarding test strength scores, Orca mini 13B, Stable Vicuna 13B, and WizardLM 13B-V1.1 excel in specific classes, achieving a notable 100\% score. Overall, EvoSuite maintains superiority in test strength across various scenarios.
  \item In exploring RQ3, we investigated how EvoSuite's performance is affected by incorporating EvoOracle-generated assertions. EvoSuite, as the baseline, demonstrated strong line coverage (71.23\%), mutation coverage (52.69\%), and test strength (75.80\%). Integrating EvoOracle assertions from various models led to nuanced impacts. While MPT-7B-Chat and Nous-hermes-13B ggml v3 showed minimal changes, combinations with Orca mini 13B-GGML v3 and WizardLM-13B-V1.1 resulted in more substantial reductions in line and mutation coverage. The overall analysis suggests that adding EvoOracle assertions to EvoSuite does not consistently improve performance across different classes and models. EvoSuite maintains robust mutant detection capabilities, with varying degrees of impact from complementary LLMs observed in specific scenarios.
\end{itemize}

\section{Future Work}
\label{sec:future_work}
\vspace{0.2 cm}

The journey of our research extends beyond the current findings, venturing into unexplored territories of innovation and improvement. An essential area of exploration lies in the improvement of the assertion generation process. As we aim to enhance the usability and adoption of LLM-generated assertions, integrating them seamlessly into continuous testing pipelines emerges as a promising avenue. However, this involves overcoming challenges related to scalability, parallelization, and real-time responsiveness, aligning with the dynamic nature of agile development practices.

In our quest for more effective assertion generation, the prospect of fine-tuning LLMs specifically for software testing contexts captures attention. Tailoring models to recognize and generate assertions in alignment with testing requirements may hold the key to enhancing precision and relevance. Addressing the limitations observed in this study requires dedicated efforts in model refinement, with a focus on developing specialized training strategies that align LLMs more closely with the intricacies of code-related tasks.

Furthermore, the potential integration of LLM-generated assertions into developers' daily workflows calls for a holistic approach. This involves developing more intuitive interfaces, visualizations, and feedback mechanisms that facilitate seamless collaboration between developers and LLMs. User-friendly integration is paramount, requiring a delicate balance between advanced technologies and practical usability.

As we look towards the future, investigating adaptive strategies for dynamically adjusting LLM configurations during the assertion generation process stands out as a crucial research direction. This entails leveraging machine learning techniques to optimize settings based on evolving testing needs, providing a dynamic and responsive framework for assertion generation.

Another dimension of exploration involves delving into methodologies for incorporating developer intent and context explicitly into the assertion generation process. Interactive prompts or mechanisms that empower developers to guide LLMs in generating assertions aligned with their testing goals present an exciting avenue for enhancing the relevance and applicability of generated assertions.

Conducting in-depth user studies to assess the long-term usability and acceptance of LLM-generated assertions in real-world development scenarios becomes imperative. Understanding how developers seamlessly incorporate LLM-generated assertions into their routine testing practices offers valuable insights into the practical implications of our research.

To broaden the impact and relevance of our approach, extending language and framework support for LLM-based assertion generation to encompass a broader spectrum of programming languages and testing frameworks becomes a priority. This ensures that the benefits and insights derived from our research can be applied across diverse software development ecosystems.

In conclusion, our journey into the realms of automated testing with LLMs serves as a foundation for continuous exploration and innovation. The dynamic intersection of artificial intelligence and software engineering presents boundless opportunities for reshaping the landscape of testing practices, and our ongoing commitment is to contribute meaningfully to this evolving narrative.